{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NQNy1AaJZ97qpLBejbzwkSjOit2qgoob",
      "authorship_tag": "ABX9TyN+RGTVf/9/5duZolcyPkhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierryFotabong/2026-AI-College-Jobs/blob/main/Vito_ProjectDataScience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "W5h5DLtGjpPT",
        "outputId": "6bee30f6-7801-474e-c52f-2d9d4ba9bdc0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3879136935.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3879136935.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    write the code to get the dataset from a shared google drive\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "write the code to get the dataset from a shared google drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5435a51"
      },
      "source": [
        "To access files from a shared Google Drive in Google Colab, you can use the following steps:\n",
        "\n",
        "1. **Mount your Google Drive:**\n",
        "   Run the following code cell to mount your Google Drive. You will be prompted to authorize Colab to access your Drive.\n",
        "\n",
        "2. **Navigate to the shared folder:**\n",
        "   Once your Drive is mounted, you can access your files through the file explorer in the left sidebar or by using shell commands in a code cell. Shared folders are usually located in the `drive/MyDrive/Shared with me/` directory.\n",
        "\n",
        "3. **Read the dataset:**\n",
        "   Use appropriate libraries (e.g., pandas) to read your dataset from the specified path in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'path/to/your/local_dataset.csv' with the actual path to your file on your local machine\n",
        "try:\n",
        "    operational_data = pd.read_csv('Users\\thierryforsackfotabong\\Downloads\\drive-download-20251004T142152Z-1-001\\operational_metrics.csv')\n",
        "    print(\"Dataset loaded successfully from local drive!\")\n",
        "    display(operational_data.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset not found at the specified local path. Please check the file path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBypBzoVkM0M",
        "outputId": "786c061f-1e73-4e9c-d8f3-fae76a4ee3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "/tmp/ipython-input-2027443531.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  operational_data = pd.read_csv('Users\\thierryforsackfotabong\\Downloads\\drive-download-20251004T142152Z-1-001\\operational_metrics.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Dataset not found at the specified local path. Please check the file path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aXI3hz7mxQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "seismic_data = files.upload()\n",
        "\n",
        "for fn in seismic_data.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(seismic_data[fn])))\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  df_seismic_data = pd.read_csv(io.StringIO(seismic_data[fn].decode('utf-8')))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "display(df_seismic_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "2QrdmXVooYwT",
        "outputId": "57fd037d-c8f1-48ee-94c6-c17a1aead3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6d6e3b0-4e4e-4f5b-badb-c83d400a1300\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6d6e3b0-4e4e-4f5b-badb-c83d400a1300\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving seismic_events.csv to seismic_events.csv\n",
            "User uploaded file \"seismic_events.csv\" with length 90600 bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               occurred_at  event_id  phase  is_producing  \\\n",
              "0  2018-12-05 05:29:53.740         3   0.01          True   \n",
              "1  2018-12-06 12:18:08.140         9   0.01          True   \n",
              "2  2018-12-07 07:03:55.513        14   0.01          True   \n",
              "3  2018-12-07 07:04:08.897        15   0.01          True   \n",
              "4  2018-12-07 07:04:17.413        16   0.01          True   \n",
              "\n",
              "      phase_started_at phase_production_ended_at       phase_ended_at  \\\n",
              "0  2018-12-04 12:00:00       2018-12-11 12:29:00  2018-12-11 12:30:00   \n",
              "1  2018-12-04 12:00:00       2018-12-11 12:29:00  2018-12-11 12:30:00   \n",
              "2  2018-12-04 12:00:00       2018-12-11 12:29:00  2018-12-11 12:30:00   \n",
              "3  2018-12-04 12:00:00       2018-12-11 12:29:00  2018-12-11 12:30:00   \n",
              "4  2018-12-04 12:00:00       2018-12-11 12:29:00  2018-12-11 12:30:00   \n",
              "\n",
              "                                     location   pgv_max  magnitude  \\\n",
              "0  POINT Z (201946.428571 214650 -3815.47619)  0.013925   0.197000   \n",
              "1  POINT Z (202005.952381 214650 -3815.47619)  0.017326   0.396200   \n",
              "2        POINT Z (202125 214230 -4053.571429)  0.009658  -0.002200   \n",
              "3        POINT Z (202125 214170 -3577.380952)  0.011502   0.068001   \n",
              "4  POINT Z (202184.52381 214050 -3755.952381)  0.013325  -0.101800   \n",
              "\n",
              "   hourly_seismicity_rate  distance_to_fault              x         y  \\\n",
              "0                       0        1255.751385  201946.428571  214650.0   \n",
              "1                       0        1206.415505  202005.952381  214650.0   \n",
              "2                       0        1365.234569  202125.000000  214230.0   \n",
              "3                       0        1192.798298  202125.000000  214170.0   \n",
              "4                       0        1262.362209  202184.523810  214050.0   \n",
              "\n",
              "             z  \n",
              "0 -3815.476190  \n",
              "1 -3815.476190  \n",
              "2 -4053.571429  \n",
              "3 -3577.380952  \n",
              "4 -3755.952381  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5190ca4-920a-43b1-910e-9a77d43a2fe8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>occurred_at</th>\n",
              "      <th>event_id</th>\n",
              "      <th>phase</th>\n",
              "      <th>is_producing</th>\n",
              "      <th>phase_started_at</th>\n",
              "      <th>phase_production_ended_at</th>\n",
              "      <th>phase_ended_at</th>\n",
              "      <th>location</th>\n",
              "      <th>pgv_max</th>\n",
              "      <th>magnitude</th>\n",
              "      <th>hourly_seismicity_rate</th>\n",
              "      <th>distance_to_fault</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-12-05 05:29:53.740</td>\n",
              "      <td>3</td>\n",
              "      <td>0.01</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-12-04 12:00:00</td>\n",
              "      <td>2018-12-11 12:29:00</td>\n",
              "      <td>2018-12-11 12:30:00</td>\n",
              "      <td>POINT Z (201946.428571 214650 -3815.47619)</td>\n",
              "      <td>0.013925</td>\n",
              "      <td>0.197000</td>\n",
              "      <td>0</td>\n",
              "      <td>1255.751385</td>\n",
              "      <td>201946.428571</td>\n",
              "      <td>214650.0</td>\n",
              "      <td>-3815.476190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-12-06 12:18:08.140</td>\n",
              "      <td>9</td>\n",
              "      <td>0.01</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-12-04 12:00:00</td>\n",
              "      <td>2018-12-11 12:29:00</td>\n",
              "      <td>2018-12-11 12:30:00</td>\n",
              "      <td>POINT Z (202005.952381 214650 -3815.47619)</td>\n",
              "      <td>0.017326</td>\n",
              "      <td>0.396200</td>\n",
              "      <td>0</td>\n",
              "      <td>1206.415505</td>\n",
              "      <td>202005.952381</td>\n",
              "      <td>214650.0</td>\n",
              "      <td>-3815.476190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-12-07 07:03:55.513</td>\n",
              "      <td>14</td>\n",
              "      <td>0.01</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-12-04 12:00:00</td>\n",
              "      <td>2018-12-11 12:29:00</td>\n",
              "      <td>2018-12-11 12:30:00</td>\n",
              "      <td>POINT Z (202125 214230 -4053.571429)</td>\n",
              "      <td>0.009658</td>\n",
              "      <td>-0.002200</td>\n",
              "      <td>0</td>\n",
              "      <td>1365.234569</td>\n",
              "      <td>202125.000000</td>\n",
              "      <td>214230.0</td>\n",
              "      <td>-4053.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-12-07 07:04:08.897</td>\n",
              "      <td>15</td>\n",
              "      <td>0.01</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-12-04 12:00:00</td>\n",
              "      <td>2018-12-11 12:29:00</td>\n",
              "      <td>2018-12-11 12:30:00</td>\n",
              "      <td>POINT Z (202125 214170 -3577.380952)</td>\n",
              "      <td>0.011502</td>\n",
              "      <td>0.068001</td>\n",
              "      <td>0</td>\n",
              "      <td>1192.798298</td>\n",
              "      <td>202125.000000</td>\n",
              "      <td>214170.0</td>\n",
              "      <td>-3577.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-12-07 07:04:17.413</td>\n",
              "      <td>16</td>\n",
              "      <td>0.01</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-12-04 12:00:00</td>\n",
              "      <td>2018-12-11 12:29:00</td>\n",
              "      <td>2018-12-11 12:30:00</td>\n",
              "      <td>POINT Z (202184.52381 214050 -3755.952381)</td>\n",
              "      <td>0.013325</td>\n",
              "      <td>-0.101800</td>\n",
              "      <td>0</td>\n",
              "      <td>1262.362209</td>\n",
              "      <td>202184.523810</td>\n",
              "      <td>214050.0</td>\n",
              "      <td>-3755.952381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5190ca4-920a-43b1-910e-9a77d43a2fe8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5190ca4-920a-43b1-910e-9a77d43a2fe8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5190ca4-920a-43b1-910e-9a77d43a2fe8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5f7953ec-382a-4203-8f40-54669585b419\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f7953ec-382a-4203-8f40-54669585b419')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5f7953ec-382a-4203-8f40-54669585b419 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_seismic_data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"occurred_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2018-12-06 12:18:08.140\",\n          \"2018-12-07 07:04:17.413\",\n          \"2018-12-07 07:03:55.513\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 3,\n        \"max\": 16,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9,\n          16,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phase\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.01,\n        \"max\": 0.01,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_producing\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phase_started_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2018-12-04 12:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phase_production_ended_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2018-12-11 12:29:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phase_ended_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2018-12-11 12:30:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"POINT Z (202005.952381 214650 -3815.47619)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pgv_max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002871459702214054,\n        \"min\": 0.0096577778458595,\n        \"max\": 0.0173259545117616,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0173259545117616\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19279590897768623,\n        \"min\": -0.1017999999999999,\n        \"max\": 0.3962,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3962\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hourly_seismicity_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"distance_to_fault\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67.86842977718658,\n        \"min\": 1192.7982984502053,\n        \"max\": 1365.2345690466611,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1206.415505377808\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 97.80759981966412,\n        \"min\": 201946.428571,\n        \"max\": 202184.52381,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          202005.952381\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 281.42494558940575,\n        \"min\": 214050.0,\n        \"max\": 214650.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          214230.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 170.45025101686423,\n        \"min\": -4053.571429,\n        \"max\": -3577.380952,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -4053.571429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_seismic_data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "8k95seoW5I_3",
        "outputId": "dc13a2af-5cb5-4413-c754-d54e1e156f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 378 entries, 0 to 377\n",
            "Data columns (total 15 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   occurred_at                378 non-null    object \n",
            " 1   event_id                   378 non-null    int64  \n",
            " 2   phase                      378 non-null    float64\n",
            " 3   is_producing               378 non-null    bool   \n",
            " 4   phase_started_at           378 non-null    object \n",
            " 5   phase_production_ended_at  378 non-null    object \n",
            " 6   phase_ended_at             378 non-null    object \n",
            " 7   location                   378 non-null    object \n",
            " 8   pgv_max                    378 non-null    float64\n",
            " 9   magnitude                  378 non-null    float64\n",
            " 10  hourly_seismicity_rate     378 non-null    int64  \n",
            " 11  distance_to_fault          378 non-null    float64\n",
            " 12  x                          378 non-null    float64\n",
            " 13  y                          378 non-null    float64\n",
            " 14  z                          378 non-null    float64\n",
            "dtypes: bool(1), float64(7), int64(2), object(5)\n",
            "memory usage: 41.8+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_operational_data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "8Ny-7NqN9OTT",
        "outputId": "6ab24365-5273-4309-f79b-7b9d442b8438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 695625 entries, 0 to 695624\n",
            "Data columns (total 25 columns):\n",
            " #   Column                     Non-Null Count   Dtype  \n",
            "---  ------                     --------------   -----  \n",
            " 0   recorded_at                695625 non-null  object \n",
            " 1   phase                      695625 non-null  float64\n",
            " 2   inj_flow                   695278 non-null  float64\n",
            " 3   inj_whp                    689533 non-null  float64\n",
            " 4   inj_temp                   689492 non-null  float64\n",
            " 5   inj_ap                     682699 non-null  float64\n",
            " 6   prod_temp                  689494 non-null  float64\n",
            " 7   prod_whp                   689538 non-null  float64\n",
            " 8   gt03_whp                   695343 non-null  float64\n",
            " 9   hedh_thpwr                 654494 non-null  float64\n",
            " 10  basin_flow                 683659 non-null  float64\n",
            " 11  prod_flow                  683344 non-null  float64\n",
            " 12  source                     695625 non-null  object \n",
            " 13  is_producing               695625 non-null  bool   \n",
            " 14  phase_started_at           695625 non-null  object \n",
            " 15  phase_production_ended_at  695625 non-null  object \n",
            " 16  phase_ended_at             695625 non-null  object \n",
            " 17  volume                     695278 non-null  float64\n",
            " 18  cum_volume                 695278 non-null  float64\n",
            " 19  inj_energy                 689522 non-null  float64\n",
            " 20  cum_inj_energy             689522 non-null  float64\n",
            " 21  cooling_energy             689481 non-null  float64\n",
            " 22  cum_cooling_energy         689481 non-null  float64\n",
            " 23  heat_exch_energy           654494 non-null  float64\n",
            " 24  cum_heat_exch_energy       654494 non-null  float64\n",
            "dtypes: bool(1), float64(19), object(5)\n",
            "memory usage: 128.0+ MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "\n",
        "class SeismicOperationalMergerV2:\n",
        "    \"\"\"\n",
        "    Merge 5-minute operational TS (left/base) with seismic events (right),\n",
        "    with minimal loss and no spurious propagation.\n",
        "    Strategy:\n",
        "      1) Parse timestamps and normalize tz.\n",
        "      2) Snap seismic events to nearest 5-min bin and aggregate per bin.\n",
        "      3) Left-merge aggregated seismic features onto operational grid.\n",
        "      4) Add clean indicators + optional rolling stats.\n",
        "      5) Phase sanity checks (diagnostic).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, operational_df: pd.DataFrame, seismic_df: pd.DataFrame):\n",
        "        self.op = operational_df.copy()\n",
        "        self.ev = seismic_df.copy()\n",
        "        self.merged = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_dt(s: pd.Series) -> pd.Series:\n",
        "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
        "        # Make tz-naive if any timezone exists\n",
        "        if getattr(dt.dt, \"tz\", None) is not None:\n",
        "            dt = dt.dt.tz_localize(None)\n",
        "        return dt\n",
        "\n",
        "    def prepare(self):\n",
        "        # ---- Parse times per your schemas ----\n",
        "        # Operational\n",
        "        self.op[\"recorded_at\"] = self._to_dt(self.op[\"recorded_at\"])\n",
        "        for c in [\"phase_started_at\", \"phase_production_ended_at\", \"phase_ended_at\"]:\n",
        "            if c in self.op.columns:\n",
        "                self.op[c] = self._to_dt(self.op[c])\n",
        "\n",
        "        # Seismic\n",
        "        self.ev[\"occurred_at\"] = self._to_dt(self.ev[\"occurred_at\"])\n",
        "        for c in [\"phase_started_at\", \"phase_production_ended_at\", \"phase_ended_at\"]:\n",
        "            if c in self.ev.columns:\n",
        "                self.ev[c] = self._to_dt(self.ev[c])\n",
        "\n",
        "        # Sort\n",
        "        self.op = self.op.sort_values(\"recorded_at\").reset_index(drop=True)\n",
        "        self.ev = self.ev.sort_values(\"occurred_at\").reset_index(drop=True)\n",
        "\n",
        "        # Basic range sanity\n",
        "        # (optional) print(self.op[\"recorded_at\"].min(), self.op[\"recorded_at\"].max())\n",
        "        # (optional) print(self.ev[\"occurred_at\"].min(), self.ev[\"occurred_at\"].max())\n",
        "\n",
        "    def aggregate_events_to_5min(self):\n",
        "        \"\"\"\n",
        "        Snap each event to its nearest 5-minute bin (±2.5 min) and aggregate events\n",
        "        that fall into the same bin. This guarantees a single seismic row per 5-min\n",
        "        timestamp and prevents duplicate merges.\n",
        "        \"\"\"\n",
        "        # Snap to nearest 5-min bin (12:02 -> 12:00, 12:03 -> 12:05, ties go to nearest-even rule)\n",
        "        self.ev[\"event_bin\"] = self.ev[\"occurred_at\"].dt.round(\"5min\")\n",
        "\n",
        "        # Aggregate per bin – choose robust summaries\n",
        "        # Keep counts and severity; you can add more aggregates as needed\n",
        "        agg = (\n",
        "            self.ev\n",
        "            .groupby(\"event_bin\")\n",
        "            .agg(\n",
        "                event_count=(\"event_id\", \"count\"),\n",
        "                event_ids=(\"event_id\", lambda s: \",\".join(map(str, s))),\n",
        "                max_magnitude=(\"magnitude\", \"max\"),\n",
        "                sum_pgv_max=(\"pgv_max\", \"sum\"),\n",
        "                mean_distance_to_fault=(\"distance_to_fault\", \"mean\"),\n",
        "                # keep a representative event for reference (e.g., max magnitude)\n",
        "                ref_event_id=(\"event_id\", lambda s: s.iloc[np.argmax(self.ev.loc[s.index, \"magnitude\"].values)]),\n",
        "                ref_magnitude=(\"magnitude\", \"max\"),\n",
        "                ref_occurred_at=(\"occurred_at\", \"min\"),  # earliest in bin\n",
        "            )\n",
        "            .reset_index()\n",
        "            .rename(columns={\"event_bin\": \"recorded_at\"})  # to match op timeline\n",
        "        )\n",
        "\n",
        "        self.ev_5min = agg\n",
        "\n",
        "    def merge(self):\n",
        "        \"\"\"\n",
        "        Left-merge seismic aggregates into the 5-min operational grid.\n",
        "        This preserves ALL op rows and attaches event features only where they occurred.\n",
        "        \"\"\"\n",
        "        self.merged = (\n",
        "            self.op.merge(self.ev_5min, on=\"recorded_at\", how=\"left\", validate=\"one_to_one\")\n",
        "        )\n",
        "\n",
        "        # Clean indicators\n",
        "        self.merged[\"event_occurred\"] = (self.merged[\"event_count\"].fillna(0) > 0).astype(\"int8\")\n",
        "\n",
        "        # Optional: fill seismic aggregates with neutral values when no event\n",
        "        fill_map = {\n",
        "            \"event_count\": 0,\n",
        "            \"max_magnitude\": np.nan,     # keep NaN to distinguish \"no event\" vs \"0.0\"\n",
        "            \"sum_pgv_max\": 0.0,\n",
        "            \"mean_distance_to_fault\": np.nan,\n",
        "            \"ref_magnitude\": np.nan,\n",
        "        }\n",
        "        for c, v in fill_map.items():\n",
        "            if c in self.merged.columns:\n",
        "                self.merged[c] = self.merged[c].astype(\"float32\").fillna(v)\n",
        "\n",
        "        # Keep event_ids as string with NaN where none\n",
        "        if \"event_ids\" in self.merged.columns:\n",
        "            self.merged[\"event_ids\"] = self.merged[\"event_ids\"].astype(\"object\")\n",
        "\n",
        "        # Downcast only the new indicator\n",
        "        self.merged[\"event_occurred\"] = self.merged[\"event_occurred\"].astype(\"int8\")\n",
        "\n",
        "    def add_nearest_event_delta(self, use_nearest_asof=True):\n",
        "        \"\"\"\n",
        "        (Optional) Attach the truly nearest event (within 150s) and time deltas.\n",
        "        This does NOT propagate events; each op row either matches a single nearest event\n",
        "        or gets NaN if the event is too far from the bin center.\n",
        "        \"\"\"\n",
        "        if not use_nearest_asof or self.ev.empty:\n",
        "            return\n",
        "\n",
        "        # asof needs sorted keys\n",
        "        op_sorted = self.merged[[\"recorded_at\"]].sort_values(\"recorded_at\").copy()\n",
        "        ev_sorted = self.ev[[\"occurred_at\", \"event_id\", \"magnitude\", \"pgv_max\"]].sort_values(\"occurred_at\").copy()\n",
        "\n",
        "        nearest = pd.merge_asof(\n",
        "            op_sorted,\n",
        "            ev_sorted,\n",
        "            left_on=\"recorded_at\",\n",
        "            right_on=\"occurred_at\",\n",
        "            direction=\"nearest\",\n",
        "            tolerance=pd.Timedelta(seconds=150),  # ±2.5 minutes\n",
        "        )\n",
        "\n",
        "        # Bring back in original order\n",
        "        nearest = nearest.set_index(op_sorted.index).reindex(self.merged.index)\n",
        "\n",
        "        self.merged[\"nearest_event_id\"] = nearest[\"event_id\"]\n",
        "        self.merged[\"nearest_event_magnitude\"] = nearest[\"magnitude\"]\n",
        "        self.merged[\"nearest_event_pgv_max\"] = nearest[\"pgv_max\"]\n",
        "        self.merged[\"delta_seconds_to_nearest_event\"] = (\n",
        "            (nearest[\"occurred_at\"] - self.merged[\"recorded_at\"]).dt.total_seconds()\n",
        "        ).astype(\"float32\")\n",
        "\n",
        "    def add_phase_sanity_flags(self):\n",
        "        \"\"\"\n",
        "        Diagnostic: did the (nearest) event occur within this row's operational phase window?\n",
        "        Uses op phase windows; if absent, skips.\n",
        "        \"\"\"\n",
        "        req = {\"phase_started_at\", \"phase_ended_at\"}\n",
        "        if not req.issubset(self.merged.columns):\n",
        "            return\n",
        "\n",
        "        # Prefer exact ref_occurred_at if present; else nearest\n",
        "        if \"ref_occurred_at\" in self.merged.columns:\n",
        "            ev_time = self.merged[\"ref_occurred_at\"]\n",
        "        elif \"delta_seconds_to_nearest_event\" in self.merged.columns:\n",
        "            # reconstruct nearest occurred_at\n",
        "            ev_time = self.merged[\"recorded_at\"] + pd.to_timedelta(self.merged[\"delta_seconds_to_nearest_event\"], unit=\"s\")\n",
        "        else:\n",
        "            # No event times available; create empty column\n",
        "            self.merged[\"seismic_during_phase\"] = np.int8(0)\n",
        "            return\n",
        "\n",
        "        self.merged[\"seismic_during_phase\"] = (\n",
        "            (ev_time.notna()) &\n",
        "            (ev_time >= self.merged[\"phase_started_at\"]) &\n",
        "            (ev_time <= self.merged[\"phase_ended_at\"])\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "    def add_simple_rollups(self, window_hours=24):\n",
        "        \"\"\"\n",
        "        Rolling aggregates over the ops timeline.\n",
        "        Note: rolling on sparse event signals; leave NA where no events.\n",
        "        \"\"\"\n",
        "        df = self.merged.set_index(\"recorded_at\").sort_index()\n",
        "\n",
        "        # Rolling count of event bins (not total events) in last window\n",
        "        if \"event_occurred\" in df.columns:\n",
        "            df[\"roll_event_bins\"] = (\n",
        "                df[\"event_occurred\"]\n",
        "                .rolling(f\"{window_hours}H\", min_periods=1)\n",
        "                .sum()\n",
        "                .astype(\"float32\")\n",
        "            )\n",
        "\n",
        "        # Rolling max magnitude observed in last window (based on max_magnitude per bin)\n",
        "        if \"max_magnitude\" in df.columns:\n",
        "            df[\"roll_max_magnitude\"] = (\n",
        "                df[\"max_magnitude\"]\n",
        "                .rolling(f\"{window_hours}H\", min_periods=1)\n",
        "                .max()\n",
        "                .astype(\"float32\")\n",
        "            )\n",
        "\n",
        "        self.merged = df.reset_index()\n",
        "\n",
        "    def run(self, window_hours=24, attach_nearest=True):\n",
        "        self.prepare()\n",
        "        self.aggregate_events_to_5min()\n",
        "        self.merge()\n",
        "        if attach_nearest:\n",
        "            self.add_nearest_event_delta(use_nearest_asof=True)\n",
        "        self.add_phase_sanity_flags()\n",
        "        self.add_simple_rollups(window_hours=window_hours)\n",
        "        return self.merged\n",
        "\n",
        "# ======================\n",
        "# Usage\n",
        "# ======================\n",
        "# op = pd.read_csv(\"operational_data.csv\")\n",
        "# ev = pd.read_csv(\"seismic_events.csv\")\n",
        "# merged = SeismicOperationalMergerV2(op, ev).run(window_hours=24, attach_nearest=True)\n",
        "# merged.to_parquet(\"merged_operational_seismic.parquet\", compression=\"snappy\")\n"
      ],
      "metadata": {
        "id": "r6xO0gzXCSe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op = df_operational_data\n",
        "ev = df_seismic_data\n",
        "merged = SeismicOperationalMergerV2(op, ev).run(window_hours=24, attach_nearest=True)\n",
        "merged.to_parquet(\"merged_operational_seismic.parquet\", compression=\"snappy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0dEQsmWCrEp",
        "outputId": "aa0fa37f-f44f-4e6f-91b2-ad6f60b8c721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-460598239.py:183: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  .rolling(f\"{window_hours}H\", min_periods=1)\n",
            "/tmp/ipython-input-460598239.py:192: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  .rolling(f\"{window_hours}H\", min_periods=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity: how many bins have events?\n",
        "merged[\"event_count\"].gt(0).sum(), ev[\"event_id\"].nunique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz_oONxdCYfj",
        "outputId": "3f3c3895-5337-4cf3-bce1-9ed91f74c18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.int64(348), 378)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "wvHMVXvwDppR",
        "outputId": "fb636545-889c-4baa-830c-129d5dc2ecde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          recorded_at  phase   inj_flow    inj_whp   inj_temp  inj_ap  \\\n",
              "0 2018-11-28 11:20:00    0.0   3.473958  29.742170  18.670664     0.0   \n",
              "1 2018-11-28 11:25:00    0.0  25.510803  29.709707  11.027036     0.0   \n",
              "2 2018-11-28 11:30:00    0.0   1.215181  29.675275  10.577293     0.0   \n",
              "3 2018-11-28 11:35:00    0.0   8.490693  29.644622  10.619303     0.0   \n",
              "4 2018-11-28 11:40:00    0.0   0.057919  29.614782  10.622052     0.0   \n",
              "\n",
              "   prod_temp  prod_whp  gt03_whp  hedh_thpwr  ...  ref_magnitude  \\\n",
              "0  22.161097  5.309335       0.0         NaN  ...            NaN   \n",
              "1  22.115342  5.307418       0.0         NaN  ...            NaN   \n",
              "2  22.089825  5.306586       0.0         NaN  ...            NaN   \n",
              "3  22.057563  5.306062       0.0         NaN  ...            NaN   \n",
              "4  22.148908  5.306007       0.0         NaN  ...            NaN   \n",
              "\n",
              "   ref_occurred_at event_occurred  nearest_event_id nearest_event_magnitude  \\\n",
              "0              NaT              0               NaN                     NaN   \n",
              "1              NaT              0               NaN                     NaN   \n",
              "2              NaT              0               NaN                     NaN   \n",
              "3              NaT              0               NaN                     NaN   \n",
              "4              NaT              0               NaN                     NaN   \n",
              "\n",
              "  nearest_event_pgv_max delta_seconds_to_nearest_event  seismic_during_phase  \\\n",
              "0                   NaN                            NaN                     0   \n",
              "1                   NaN                            NaN                     0   \n",
              "2                   NaN                            NaN                     0   \n",
              "3                   NaN                            NaN                     0   \n",
              "4                   NaN                            NaN                     0   \n",
              "\n",
              "   roll_event_bins  roll_max_magnitude  \n",
              "0              0.0                 NaN  \n",
              "1              0.0                 NaN  \n",
              "2              0.0                 NaN  \n",
              "3              0.0                 NaN  \n",
              "4              0.0                 NaN  \n",
              "\n",
              "[5 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a8b86f2-dd98-4946-b95c-8e9a38c91527\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recorded_at</th>\n",
              "      <th>phase</th>\n",
              "      <th>inj_flow</th>\n",
              "      <th>inj_whp</th>\n",
              "      <th>inj_temp</th>\n",
              "      <th>inj_ap</th>\n",
              "      <th>prod_temp</th>\n",
              "      <th>prod_whp</th>\n",
              "      <th>gt03_whp</th>\n",
              "      <th>hedh_thpwr</th>\n",
              "      <th>...</th>\n",
              "      <th>ref_magnitude</th>\n",
              "      <th>ref_occurred_at</th>\n",
              "      <th>event_occurred</th>\n",
              "      <th>nearest_event_id</th>\n",
              "      <th>nearest_event_magnitude</th>\n",
              "      <th>nearest_event_pgv_max</th>\n",
              "      <th>delta_seconds_to_nearest_event</th>\n",
              "      <th>seismic_during_phase</th>\n",
              "      <th>roll_event_bins</th>\n",
              "      <th>roll_max_magnitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-11-28 11:20:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.473958</td>\n",
              "      <td>29.742170</td>\n",
              "      <td>18.670664</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.161097</td>\n",
              "      <td>5.309335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-11-28 11:25:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.510803</td>\n",
              "      <td>29.709707</td>\n",
              "      <td>11.027036</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.115342</td>\n",
              "      <td>5.307418</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-11-28 11:30:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.215181</td>\n",
              "      <td>29.675275</td>\n",
              "      <td>10.577293</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.089825</td>\n",
              "      <td>5.306586</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-11-28 11:35:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.490693</td>\n",
              "      <td>29.644622</td>\n",
              "      <td>10.619303</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.057563</td>\n",
              "      <td>5.306062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-11-28 11:40:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057919</td>\n",
              "      <td>29.614782</td>\n",
              "      <td>10.622052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.148908</td>\n",
              "      <td>5.306007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 41 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a8b86f2-dd98-4946-b95c-8e9a38c91527')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a8b86f2-dd98-4946-b95c-8e9a38c91527 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a8b86f2-dd98-4946-b95c-8e9a38c91527');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9cabdfe2-7bfc-4989-ace5-2e88b8a40e58\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9cabdfe2-7bfc-4989-ace5-2e88b8a40e58')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9cabdfe2-7bfc-4989-ace5-2e88b8a40e58 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged.to_csv(\"/content/drive/MyDrive/merged_operational_seismic.csv\", index=False)"
      ],
      "metadata": {
        "id": "h_57GHYMEcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import React, { useState } from 'react';\n",
        "import { ChevronDown, ChevronRight, Calendar, Clock, Users, AlertCircle } from 'lucide-react';\n",
        "\n",
        "const JiraProjectPlan = () => {\n",
        "  const [expandedEpics, setExpandedEpics] = useState({});\n",
        "  const [expandedStories, setExpandedStories] = useState({});\n",
        "  const [selectedSprint, setSelectedSprint] = useState('all');\n",
        "\n",
        "  const projectData = {\n",
        "    epics: [\n",
        "      {\n",
        "        id: 'SEISMIC-1',\n",
        "        name: 'Data Understanding & Exploration',\n",
        "        sprint: 'Sprint 1-2',\n",
        "        duration: '3 weeks',\n",
        "        storyPoints: 55,\n",
        "        color: 'bg-blue-100 border-blue-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-101',\n",
        "            title: 'Data Quality Assessment & Validation',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 1',\n",
        "            labels: ['data-quality', 'foundation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-101.1', title: 'Load and inspect all datasets (operational + seismic)', estimate: '4h', description: 'Import CSV/database files, run .info(), .describe(), check dtypes' },\n",
        "              { id: 'SEISMIC-101.2', title: 'Create missing data heatmap and analysis', estimate: '3h', description: 'Use missingno library, calculate % missing per column, identify patterns' },\n",
        "              { id: 'SEISMIC-101.3', title: 'Implement outlier detection pipeline', estimate: '6h', description: 'Box plots, z-scores, IQR method, flag impossible values' },\n",
        "              { id: 'SEISMIC-101.4', title: 'Validate temporal coverage and sampling frequency', estimate: '4h', description: 'Plot availability timeline, calculate uptime %, identify gaps >1hr' },\n",
        "              { id: 'SEISMIC-101.5', title: 'Document GT03_whp and heat exchanger data gaps', estimate: '2h', description: 'Specific analysis of known problematic sensors' },\n",
        "              { id: 'SEISMIC-101.6', title: 'Build automated data quality dashboard', estimate: '8h', description: 'Create Plotly/Dash dashboard with completeness scores, outlier counts' },\n",
        "              { id: 'SEISMIC-101.7', title: 'Write data validation scripts for reusability', estimate: '5h', description: 'Modular Python functions for ongoing data checks' },\n",
        "              { id: 'SEISMIC-101.8', title: 'Produce data quality executive report', estimate: '4h', description: 'PDF report with reliability scores (0-100) per sensor' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-102',\n",
        "            title: 'Exploratory Data Analysis - Temporal Patterns',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 1',\n",
        "            labels: ['EDA', 'analysis'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-102.1', title: 'Create time series plots of seismic event frequency', estimate: '3h', description: 'Daily, weekly, monthly aggregations with trend lines' },\n",
        "              { id: 'SEISMIC-102.2', title: 'Perform seasonal decomposition (STL)', estimate: '4h', description: 'Separate trend, seasonal, and residual components' },\n",
        "              { id: 'SEISMIC-102.3', title: 'Analyze event clustering patterns', estimate: '5h', description: 'Identify swarms vs isolated events, inter-event time statistics' },\n",
        "              { id: 'SEISMIC-102.4', title: 'Calculate cumulative seismic events over project lifetime', estimate: '2h', description: 'Plot cumulative curve, identify acceleration periods' },\n",
        "              { id: 'SEISMIC-102.5', title: 'Build interactive temporal visualization dashboard', estimate: '8h', description: 'Plotly dashboard with zoom, filter, and annotation capabilities' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-103',\n",
        "            title: 'EDA - Operational-Seismic Correlations',\n",
        "            priority: 'High',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 2',\n",
        "            labels: ['EDA', 'correlation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-103.1', title: 'Create scatter plots: injection params vs events', estimate: '4h', description: 'Injection rate, pressure, temperature vs event counts' },\n",
        "              { id: 'SEISMIC-103.2', title: 'Calculate Pearson and Spearman correlations', estimate: '3h', description: 'Correlation coefficients with significance tests' },\n",
        "              { id: 'SEISMIC-103.3', title: 'Perform lagged cross-correlation analysis', estimate: '6h', description: 'Test lags from 1h to 7 days, find optimal lag times' },\n",
        "              { id: 'SEISMIC-103.4', title: 'Implement Granger causality tests', estimate: '5h', description: 'Test if operational params Granger-cause seismic activity' },\n",
        "              { id: 'SEISMIC-103.5', title: 'Build correlation matrix with hierarchical clustering', estimate: '4h', description: 'Visualize grouped correlations, identify feature families' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-104',\n",
        "            title: 'EDA - Spatial & Phase Analysis',\n",
        "            priority: 'High',\n",
        "            assignee: 'Geoscientist',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 2',\n",
        "            labels: ['EDA', 'spatial'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-104.1', title: 'Create spatial event map with magnitude markers', estimate: '5h', description: 'Plot events on facility map, size by magnitude' },\n",
        "              { id: 'SEISMIC-104.2', title: 'Generate kernel density heatmaps', estimate: '4h', description: 'KDE of event locations, overlay injection/production wells' },\n",
        "              { id: 'SEISMIC-104.3', title: 'Analyze distance_to_fault distribution', estimate: '3h', description: 'Relationship between fault distance and magnitude' },\n",
        "              { id: 'SEISMIC-104.4', title: 'Phase-based pattern comparison', estimate: '6h', description: 'Event rates in injection vs production, KS-tests' },\n",
        "              { id: 'SEISMIC-104.5', title: 'Create phase transition analysis', estimate: '4h', description: 'Events in hours following phase changes, hazard rates' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-105',\n",
        "            title: 'EDA Report & Baseline Models',\n",
        "            priority: 'High',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 2',\n",
        "            labels: ['documentation', 'baseline'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-105.1', title: 'Compile 20-30 page visualization report', estimate: '8h', description: 'Executive summary + detailed sections for each analysis' },\n",
        "              { id: 'SEISMIC-105.2', title: 'Build simple logistic regression baseline', estimate: '4h', description: 'Baseline model to beat: can we predict better than random?' },\n",
        "              { id: 'SEISMIC-105.3', title: 'Document top 5 predictive feature candidates', estimate: '3h', description: 'Findings document based on EDA insights' },\n",
        "              { id: 'SEISMIC-105.4', title: 'Present EDA findings to stakeholders', estimate: '2h', description: 'Prepare and deliver presentation' }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        id: 'SEISMIC-2',\n",
        "        name: 'Feature Engineering & Signal Processing',\n",
        "        sprint: 'Sprint 2-3',\n",
        "        duration: '2 weeks',\n",
        "        storyPoints: 50,\n",
        "        color: 'bg-purple-100 border-purple-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-201',\n",
        "            title: 'Temporal Feature Engineering Pipeline',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 2-3',\n",
        "            labels: ['feature-eng', 'temporal'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-201.1', title: 'Create rolling window statistics', estimate: '6h', description: 'Mean, std, min, max, median for all operational params' },\n",
        "              { id: 'SEISMIC-201.2', title: 'Implement rate of change features', estimate: '4h', description: 'First derivative, second derivative, percentage change' },\n",
        "              { id: 'SEISMIC-201.3', title: 'Build cumulative stress indicators', estimate: '5h', description: 'Cumulative injection volume, pressure-time integral, days since event' },\n",
        "              { id: 'SEISMIC-201.4', title: 'Generate time-based features', estimate: '3h', description: 'Hour, day of week, month, phase duration, time since phase change' },\n",
        "              { id: 'SEISMIC-201.5', title: 'Create coefficient of variation features', estimate: '2h', description: 'std/mean to capture relative variability' },\n",
        "              { id: 'SEISMIC-201.6', title: 'Build modular feature engineering pipeline', estimate: '6h', description: 'Python classes for reusable, parameterized feature creation' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-202',\n",
        "            title: 'Advanced Domain-Specific Features',\n",
        "            priority: 'High',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 3',\n",
        "            labels: ['feature-eng', 'domain'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-202.1', title: 'Calculate hydraulic energy metrics', estimate: '4h', description: 'Hydraulic power, energy injection rate, specific energy' },\n",
        "              { id: 'SEISMIC-202.2', title: 'Develop pressure differential indicators', estimate: '4h', description: 'Delta-P from baseline, normalized pressure, overshoot metrics' },\n",
        "              { id: 'SEISMIC-202.3', title: 'Create thermal stress features', estimate: '4h', description: 'Temperature difference, thermal power, thermal cycling count' },\n",
        "              { id: 'SEISMIC-202.4', title: 'Compute volumetric strain proxies', estimate: '5h', description: 'Net volume change, rate imbalance, reservoir fullness indicator' },\n",
        "              { id: 'SEISMIC-202.5', title: 'Engineer seismic history features', estimate: '5h', description: 'Event counts in rolling windows, max magnitude, energy release rate' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-203',\n",
        "            title: 'Signal Processing & Anomaly Detection',\n",
        "            priority: 'High',\n",
        "            assignee: 'Signal Processing Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 3',\n",
        "            labels: ['signal-processing', 'advanced'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-203.1', title: 'Implement continuous wavelet transform', estimate: '6h', description: 'Apply to pressure/flow, extract energy at different scales' },\n",
        "              { id: 'SEISMIC-203.2', title: 'Perform frequency domain analysis', estimate: '5h', description: 'Power spectral density, identify cyclic patterns' },\n",
        "              { id: 'SEISMIC-203.3', title: 'Implement change point detection', estimate: '5h', description: 'Detect regime changes in operations' },\n",
        "              { id: 'SEISMIC-203.4', title: 'Build anomaly detection features', estimate: '6h', description: 'Isolation Forest, LOF, autoencoder reconstruction error' },\n",
        "              { id: 'SEISMIC-203.5', title: 'Create spectral anomaly score feature', estimate: '4h', description: 'Deviation from typical frequency signature' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-204',\n",
        "            title: 'Feature Selection & Documentation',\n",
        "            priority: 'High',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 3',\n",
        "            labels: ['feature-selection', 'documentation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-204.1', title: 'Calculate feature importance using Random Forest', estimate: '3h', description: 'Run RF on all features, rank by importance' },\n",
        "              { id: 'SEISMIC-204.2', title: 'Compute mutual information scores', estimate: '3h', description: 'Mutual info between features and target' },\n",
        "              { id: 'SEISMIC-204.3', title: 'Generate SHAP values for interpretation', estimate: '4h', description: 'SHAP analysis on top features' },\n",
        "              { id: 'SEISMIC-204.4', title: 'Create correlation matrix and remove redundant features', estimate: '3h', description: 'Remove features with >0.95 correlation' },\n",
        "              { id: 'SEISMIC-204.5', title: 'Build feature selection pipeline', estimate: '4h', description: 'Reduce to top 30-50 most predictive features' },\n",
        "              { id: 'SEISMIC-204.6', title: 'Produce feature catalog document', estimate: '5h', description: 'Documentation of 100-200 features with descriptions' }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        id: 'SEISMIC-3',\n",
        "        name: 'Predictive Model Development',\n",
        "        sprint: 'Sprint 4-6',\n",
        "        duration: '4 weeks',\n",
        "        storyPoints: 80,\n",
        "        color: 'bg-green-100 border-green-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-301',\n",
        "            title: 'LSTM Model Development',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 4',\n",
        "            labels: ['model', 'lstm'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-301.1', title: 'Create sequence data preparation pipeline', estimate: '5h', description: 'Build sequences of 72 timesteps, train/val/test split' },\n",
        "              { id: 'SEISMIC-301.2', title: 'Design LSTM architecture', estimate: '4h', description: '2-layer LSTM with dropout, batch normalization' },\n",
        "              { id: 'SEISMIC-301.3', title: 'Implement class weighting', estimate: '3h', description: 'Calculate weights for rare events' },\n",
        "              { id: 'SEISMIC-301.4', title: 'Train models for multiple horizons', estimate: '12h', description: '1h, 3h, 6h, 12h, 24h ahead models' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-302',\n",
        "            title: 'XGBoost & Random Forest Models',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 4',\n",
        "            labels: ['model', 'tree-based'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-302.1', title: 'Prepare feature matrix', estimate: '3h', description: 'Flatten time series into feature vectors' },\n",
        "              { id: 'SEISMIC-302.2', title: 'Implement SMOTE for class imbalance', estimate: '4h', description: 'Oversample minority class' },\n",
        "              { id: 'SEISMIC-302.3', title: 'Train XGBoost with optimal parameters', estimate: '6h', description: 'Grid search, use scale_pos_weight' },\n",
        "              { id: 'SEISMIC-302.4', title: 'Train Random Forest ensemble', estimate: '5h', description: 'Hyperparameter tuning' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-303',\n",
        "            title: 'Advanced Deep Learning Models',\n",
        "            priority: 'High',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 5',\n",
        "            labels: ['model', 'deep-learning'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-303.1', title: 'Design CNN-LSTM hybrid', estimate: '5h', description: 'Conv1D layers + LSTM' },\n",
        "              { id: 'SEISMIC-303.2', title: 'Build Transformer architecture', estimate: '6h', description: 'Multi-head attention' },\n",
        "              { id: 'SEISMIC-303.3', title: 'Train and validate models', estimate: '8h', description: 'End-to-end training' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-304',\n",
        "            title: 'Ensemble Models',\n",
        "            priority: 'High',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 5',\n",
        "            labels: ['model', 'ensemble'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-304.1', title: 'Build stacking classifier', estimate: '5h', description: 'Stack LSTM, XGBoost, RF' },\n",
        "              { id: 'SEISMIC-304.2', title: 'Implement weighted voting', estimate: '4h', description: 'Weight by validation performance' },\n",
        "              { id: 'SEISMIC-304.3', title: 'Develop uncertainty quantification', estimate: '5h', description: 'Ensemble variance' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-305',\n",
        "            title: 'Model Evaluation',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 6',\n",
        "            labels: ['evaluation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-305.1', title: 'Create evaluation pipeline', estimate: '6h', description: 'Accuracy, precision, recall, F1, AUC-ROC' },\n",
        "              { id: 'SEISMIC-305.2', title: 'Generate confusion matrices', estimate: '3h', description: 'Visualize for all models' },\n",
        "              { id: 'SEISMIC-305.3', title: 'Build comparison dashboard', estimate: '6h', description: 'Interactive model comparison' },\n",
        "              { id: 'SEISMIC-305.4', title: 'Document best model selection', estimate: '3h', description: 'Technical report' }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        id: 'SEISMIC-4',\n",
        "        name: 'Traffic Light System Design',\n",
        "        sprint: 'Sprint 7-8',\n",
        "        duration: '2 weeks',\n",
        "        storyPoints: 45,\n",
        "        color: 'bg-yellow-100 border-yellow-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-401',\n",
        "            title: 'Risk Threshold Calibration',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 7',\n",
        "            labels: ['tls', 'calibration'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-401.1', title: 'Analyze historical data for thresholds', estimate: '5h', description: 'Optimize operational objectives' },\n",
        "              { id: 'SEISMIC-401.2', title: 'Calibrate Green-Yellow threshold', estimate: '4h', description: 'Target FPR < 20%' },\n",
        "              { id: 'SEISMIC-401.3', title: 'Calibrate Yellow-Red threshold', estimate: '4h', description: 'Based on magnitude risk' },\n",
        "              { id: 'SEISMIC-401.4', title: 'Implement multi-factor assessment', estimate: '5h', description: 'Combine probability, magnitude, uncertainty' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-402',\n",
        "            title: 'State Machine Implementation',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Software Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 7',\n",
        "            labels: ['tls', 'implementation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-402.1', title: 'Design state machine architecture', estimate: '4h', description: 'State transitions, logging' },\n",
        "              { id: 'SEISMIC-402.2', title: 'Implement TrafficLightSystem class', estimate: '6h', description: 'Python class with state management' },\n",
        "              { id: 'SEISMIC-402.3', title: 'Build alert triggering system', estimate: '5h', description: 'SMS, email for RED alerts' },\n",
        "              { id: 'SEISMIC-402.4', title: 'Write unit tests', estimate: '4h', description: 'Test all transitions' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-403',\n",
        "            title: 'Operational Recommendations Engine',\n",
        "            priority: 'High',\n",
        "            assignee: 'Process Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 8',\n",
        "            labels: ['tls', 'optimization'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-403.1', title: 'Define operational protocols', estimate: '6h', description: 'Actions for GREEN, YELLOW, RED' },\n",
        "              { id: 'SEISMIC-403.2', title: 'Build optimization algorithm', estimate: '8h', description: 'Maximize production within risk limits' },\n",
        "              { id: 'SEISMIC-403.3', title: 'Create decision trees', estimate: '6h', description: 'If-then rules for scenarios' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-404',\n",
        "            title: 'TLS Documentation',\n",
        "            priority: 'High',\n",
        "            assignee: 'Technical Writer',\n",
        "            storyPoints: 5,\n",
        "            sprint: 'Sprint 8',\n",
        "            labels: ['documentation'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-404.1', title: 'Write system specifications', estimate: '6h', description: 'Complete technical docs' },\n",
        "              { id: 'SEISMIC-404.2', title: 'Document risk methodology', estimate: '4h', description: 'Calibration rationale' },\n",
        "              { id: 'SEISMIC-404.3', title: 'Create operational protocols', estimate: '4h', description: 'Step-by-step procedures' }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        id: 'SEISMIC-5',\n",
        "        name: 'Validation & Testing',\n",
        "        sprint: 'Sprint 9-10',\n",
        "        duration: '2 weeks',\n",
        "        storyPoints: 40,\n",
        "        color: 'bg-red-100 border-red-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-501',\n",
        "            title: 'Historical Backtesting',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 9',\n",
        "            labels: ['validation', 'backtesting'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-501.1', title: 'Validate on known seismic events', estimate: '6h', description: 'Test predictions against historical events' },\n",
        "              { id: 'SEISMIC-501.2', title: 'Analyze false positives', estimate: '5h', description: 'Investigate when model predicted wrong' },\n",
        "              { id: 'SEISMIC-501.3', title: 'Analyze false negatives', estimate: '5h', description: 'Missed events analysis' },\n",
        "              { id: 'SEISMIC-501.4', title: 'Test performance by operational phase', estimate: '4h', description: 'Injection vs production accuracy' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-502',\n",
        "            title: 'Stress Testing & Robustness',\n",
        "            priority: 'High',\n",
        "            assignee: 'ML Engineer',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 9',\n",
        "            labels: ['validation', 'robustness'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-502.1', title: 'Test under extreme conditions', estimate: '5h', description: 'Edge case scenarios' },\n",
        "              { id: 'SEISMIC-502.2', title: 'Evaluate with data gaps', estimate: '4h', description: 'Missing sensor data scenarios' },\n",
        "              { id: 'SEISMIC-502.3', title: 'Assess sensor failure robustness', estimate: '4h', description: 'Graceful degradation testing' },\n",
        "              { id: 'SEISMIC-502.4', title: 'Test latency and response time', estimate: '3h', description: 'Real-time performance benchmarks' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-503',\n",
        "            title: 'Economic Impact Analysis',\n",
        "            priority: 'High',\n",
        "            assignee: 'Business Analyst',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 10',\n",
        "            labels: ['validation', 'roi'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-503.1', title: 'Calculate production loss from FP', estimate: '4h', description: 'EUR cost of false alarms' },\n",
        "              { id: 'SEISMIC-503.2', title: 'Calculate prevented damage from TP', estimate: '4h', description: 'EUR savings from correct predictions' },\n",
        "              { id: 'SEISMIC-503.3', title: 'Optimize risk-reward balance', estimate: '5h', description: 'Find optimal operating point' },\n",
        "              { id: 'SEISMIC-503.4', title: 'Build ROI projection model', estimate: '5h', description: '1-year, 3-year, 5-year projections' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-504',\n",
        "            title: 'Validation Report & Dashboard',\n",
        "            priority: 'High',\n",
        "            assignee: 'Data Scientist',\n",
        "            storyPoints: 8,\n",
        "            sprint: 'Sprint 10',\n",
        "            labels: ['validation', 'reporting'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-504.1', title: 'Create performance metrics dashboard', estimate: '6h', description: 'Interactive dashboard with all metrics' },\n",
        "              { id: 'SEISMIC-504.2', title: 'Write validation report', estimate: '8h', description: 'Comprehensive validation documentation' },\n",
        "              { id: 'SEISMIC-504.3', title: 'Prepare stakeholder presentation', estimate: '4h', description: 'Executive summary with results' }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        id: 'SEISMIC-6',\n",
        "        name: 'Implementation & Deployment',\n",
        "        sprint: 'Sprint 11-12',\n",
        "        duration: '3 weeks',\n",
        "        storyPoints: 55,\n",
        "        color: 'bg-indigo-100 border-indigo-300',\n",
        "        stories: [\n",
        "          {\n",
        "            id: 'SEISMIC-601',\n",
        "            title: 'Real-time Data Pipeline',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Data Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 11',\n",
        "            labels: ['deployment', 'pipeline'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-601.1', title: 'Design real-time data architecture', estimate: '5h', description: 'Kafka/streaming architecture design' },\n",
        "              { id: 'SEISMIC-601.2', title: 'Build data ingestion pipeline', estimate: '8h', description: 'Connect to operational sensors' },\n",
        "              { id: 'SEISMIC-601.3', title: 'Implement feature computation pipeline', estimate: '6h', description: 'Real-time feature engineering' },\n",
        "              { id: 'SEISMIC-601.4', title: 'Set up data validation checks', estimate: '4h', description: 'Quality checks on incoming data' },\n",
        "              { id: 'SEISMIC-601.5', title: 'Test pipeline throughput', estimate: '3h', description: 'Ensure 5-min latency' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-602',\n",
        "            title: 'Model Serving Infrastructure',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'MLOps Engineer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 11',\n",
        "            labels: ['deployment', 'mlops'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-602.1', title: 'Set up model serving platform', estimate: '6h', description: 'TensorFlow Serving or similar' },\n",
        "              { id: 'SEISMIC-602.2', title: 'Deploy models to production', estimate: '5h', description: 'Containerize and deploy all models' },\n",
        "              { id: 'SEISMIC-602.3', title: 'Implement prediction API', estimate: '6h', description: 'REST API for predictions' },\n",
        "              { id: 'SEISMIC-602.4', title: 'Set up model monitoring', estimate: '5h', description: 'Track prediction drift, latency' },\n",
        "              { id: 'SEISMIC-602.5', title: 'Create model rollback mechanism', estimate: '4h', description: 'Fallback to previous version' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-603',\n",
        "            title: 'Operator Dashboard Development',\n",
        "            priority: 'Highest',\n",
        "            assignee: 'Frontend Developer',\n",
        "            storyPoints: 13,\n",
        "            sprint: 'Sprint 11-12',\n",
        "            labels: ['deployment', 'ui'],\n",
        "            subtasks: [\n",
        "              { id: 'SEISMIC-603.1', title: 'Design dashboard UI/UX', estimate: '6h', description: 'Wireframes and mockups' },\n",
        "              { id: 'SEISMIC-603.2', title: 'Build traffic light display', estimate: '5h', description: 'Real-time status visualization' },\n",
        "              { id: 'SEISMIC-603.3', title: 'Create prediction timeline view', estimate: '6h', description: 'Show predictions 24h ahead' },\n",
        "              { id: 'SEISMIC-603.4', title: 'Implement alert management', estimate: '5h', description: 'View and acknowledge alerts' },\n",
        "              { id: 'SEISMIC-603.5', title: 'Build historical view', estimate: '4h', description: 'Past predictions vs actual events' }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            id: 'SEISMIC-604',\n",
        "            title: 'Alert & Notification System',"
      ],
      "metadata": {
        "id": "Zz3qm5HUpabt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "pkAYQ4i1qQv5",
        "outputId": "1196a806-e081-453d-c368-8ddbb79919b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Geothermal Time-Series Feature Correlation Analysis\n",
        "=====================================================\n",
        "Objective: Feature selection for seismic event forecasting (1-week ahead)\n",
        "Dataset: 695,625 rows × 30 features\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "filepath = \"content\\drive\\MyDrive\\operational_data_final_11 (1).csv\"\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "class GeothermalFeatureAnalysis:\n",
        "    \"\"\"\n",
        "    Comprehensive feature analysis for geothermal time-series data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        \"\"\"Load and prepare data\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"GEOTHERMAL TIME-SERIES FEATURE CORRELATION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        self.df = pd.read_csv(filepath, parse_dates=['recorded_at'])\n",
        "        print(f\"\\n✓ Loaded dataset: {self.df.shape[0]:,} rows × {self.df.shape[1]} columns\")\n",
        "\n",
        "        # Handle placeholder values\n",
        "        self.df.replace(-9999, np.nan, inplace=True)\n",
        "\n",
        "        # Identify column types\n",
        "        self.timestamp_col = 'recorded_at'\n",
        "        self.categorical_cols = ['phase', 'dataset']\n",
        "        self.target_seismic = ['has_event', 'magnitude', 'hourly_seismicity_rate']\n",
        "\n",
        "        # Get numeric columns\n",
        "        self.numeric_cols = [col for col in self.df.columns\n",
        "                            if col not in [self.timestamp_col] + self.categorical_cols\n",
        "                            and pd.api.types.is_numeric_dtype(self.df[col])]\n",
        "\n",
        "        print(f\"✓ Identified {len(self.numeric_cols)} numeric features\")\n",
        "        print(f\"✓ Seismic target variables: {self.target_seismic}\")\n",
        "\n",
        "    def data_quality_report(self):\n",
        "        \"\"\"Generate data quality report\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"1. DATA QUALITY REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Missing values\n",
        "        missing = self.df[self.numeric_cols].isnull().sum()\n",
        "        missing_pct = (missing / len(self.df) * 100).round(2)\n",
        "\n",
        "        quality_df = pd.DataFrame({\n",
        "            'Missing_Count': missing,\n",
        "            'Missing_Pct': missing_pct,\n",
        "            'Dtype': self.df[self.numeric_cols].dtypes\n",
        "        }).sort_values('Missing_Pct', ascending=False)\n",
        "\n",
        "        print(\"\\n📊 Missing Data Summary:\")\n",
        "        print(quality_df[quality_df['Missing_Pct'] > 0])\n",
        "\n",
        "        # Flag high missing rate features\n",
        "        high_missing = quality_df[quality_df['Missing_Pct'] > 20].index.tolist()\n",
        "        if high_missing:\n",
        "            print(f\"\\n⚠️  Features with >20% missing data: {high_missing}\")\n",
        "            print(\"   → Consider imputation or removal\")\n",
        "\n",
        "        return quality_df\n",
        "\n",
        "    def compute_correlations(self):\n",
        "        \"\"\"Compute Pearson and Spearman correlations\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"2. CORRELATION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Pearson correlation\n",
        "        print(\"\\n📈 Computing Pearson correlation (linear relationships)...\")\n",
        "        self.pearson_corr = self.df[self.numeric_cols].corr(method='pearson')\n",
        "\n",
        "        # Spearman correlation\n",
        "        print(\"📈 Computing Spearman correlation (monotonic relationships)...\")\n",
        "        self.spearman_corr = self.df[self.numeric_cols].corr(method='spearman')\n",
        "\n",
        "        print(f\"✓ Correlation matrices computed: {self.pearson_corr.shape}\")\n",
        "\n",
        "    def identify_multicollinearity(self, threshold=0.8):\n",
        "        \"\"\"Identify highly correlated feature pairs\"\"\"\n",
        "        print(f\"\\n🔍 Identifying multicollinearity (|r| > {threshold})...\")\n",
        "\n",
        "        # Get upper triangle\n",
        "        corr_matrix = self.pearson_corr.abs()\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "\n",
        "        # Find high correlations\n",
        "        high_corr_pairs = []\n",
        "        for col in upper_tri.columns:\n",
        "            for row in upper_tri.index:\n",
        "                val = upper_tri.loc[row, col]\n",
        "                if val > threshold:\n",
        "                    high_corr_pairs.append({\n",
        "                        'Feature_1': row,\n",
        "                        'Feature_2': col,\n",
        "                        'Correlation': val\n",
        "                    })\n",
        "\n",
        "        self.high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\n",
        "            'Correlation', ascending=False\n",
        "        )\n",
        "\n",
        "        print(f\"\\n⚠️  Found {len(self.high_corr_df)} feature pairs with |r| > {threshold}:\")\n",
        "        if len(self.high_corr_df) > 0:\n",
        "            print(self.high_corr_df.head(15))\n",
        "\n",
        "            # Identify features to consider removing\n",
        "            features_to_review = set(self.high_corr_df['Feature_1'].tolist() +\n",
        "                                    self.high_corr_df['Feature_2'].tolist())\n",
        "            print(f\"\\n📋 Features involved in high correlations: {len(features_to_review)}\")\n",
        "            for feat in list(features_to_review)[:10]:\n",
        "                print(f\"   - {feat}\")\n",
        "\n",
        "        return self.high_corr_df\n",
        "\n",
        "    def compute_vif(self):\n",
        "        \"\"\"Calculate Variance Inflation Factor\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"3. VIF ANALYSIS (Variance Inflation Factor)\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Prepare data (drop NaN)\n",
        "        df_clean = self.df[self.numeric_cols].dropna()\n",
        "\n",
        "        if len(df_clean) < 100:\n",
        "            print(\"⚠️  Insufficient complete cases for VIF calculation\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n📊 Using {len(df_clean):,} complete cases for VIF calculation...\")\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "        # Calculate VIF\n",
        "        vif_data = []\n",
        "        for i, col in enumerate(self.numeric_cols):\n",
        "            try:\n",
        "                vif = variance_inflation_factor(X_scaled, i)\n",
        "                vif_data.append({'Feature': col, 'VIF': vif})\n",
        "            except:\n",
        "                vif_data.append({'Feature': col, 'VIF': np.nan})\n",
        "\n",
        "        self.vif_df = pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
        "\n",
        "        # Interpretation\n",
        "        print(\"\\n📊 VIF Results (Top 15):\")\n",
        "        print(self.vif_df.head(15).to_string(index=False))\n",
        "\n",
        "        print(\"\\n📖 VIF Interpretation:\")\n",
        "        print(\"   VIF < 5   : Low multicollinearity ✓\")\n",
        "        print(\"   5 < VIF < 10 : Moderate multicollinearity ⚠️\")\n",
        "        print(\"   VIF > 10  : High multicollinearity ❌ (consider removal)\")\n",
        "\n",
        "        high_vif = self.vif_df[self.vif_df['VIF'] > 10]['Feature'].tolist()\n",
        "        if high_vif:\n",
        "            print(f\"\\n⚠️  Features with VIF > 10: {high_vif}\")\n",
        "\n",
        "        return self.vif_df\n",
        "\n",
        "    def target_correlation_ranking(self, target='hourly_seismicity_rate'):\n",
        "        \"\"\"Rank features by correlation with target\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"4. TARGET CORRELATION RANKING: {target}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if target not in self.numeric_cols:\n",
        "            print(f\"⚠️  Target '{target}' not found in numeric columns\")\n",
        "            return None\n",
        "\n",
        "        # Pearson correlations\n",
        "        pearson_target = self.pearson_corr[target].drop(target).abs().sort_values(ascending=False)\n",
        "\n",
        "        # Spearman correlations\n",
        "        spearman_target = self.spearman_corr[target].drop(target).abs().sort_values(ascending=False)\n",
        "\n",
        "        ranking_df = pd.DataFrame({\n",
        "            'Feature': pearson_target.index,\n",
        "            'Pearson_Corr': pearson_target.values,\n",
        "            'Spearman_Corr': [spearman_target[feat] for feat in pearson_target.index],\n",
        "            'Avg_Abs_Corr': [(pearson_target[feat] + spearman_target[feat]) / 2\n",
        "                             for feat in pearson_target.index]\n",
        "        }).sort_values('Avg_Abs_Corr', ascending=False)\n",
        "\n",
        "        print(\"\\n📊 Top 20 Features Correlated with Target:\")\n",
        "        print(ranking_df.head(20).to_string(index=False))\n",
        "\n",
        "        return ranking_df\n",
        "\n",
        "    def visualize_correlations(self):\n",
        "        \"\"\"Create comprehensive correlation visualizations\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"5. GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Figure 1: Pearson Correlation Heatmap\n",
        "        fig1, ax1 = plt.subplots(figsize=(20, 18))\n",
        "        mask = np.triu(np.ones_like(self.pearson_corr, dtype=bool))\n",
        "        sns.heatmap(self.pearson_corr, mask=mask, annot=False, fmt='.2f',\n",
        "                   cmap='coolwarm', center=0, square=True, linewidths=0.5,\n",
        "                   cbar_kws={\"shrink\": 0.8}, ax=ax1, vmin=-1, vmax=1)\n",
        "        ax1.set_title('Pearson Correlation Matrix (Linear Relationships)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pearson_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"✓ Saved: pearson_correlation_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2: Spearman Correlation Heatmap\n",
        "        fig2, ax2 = plt.subplots(figsize=(20, 18))\n",
        "        mask = np.triu(np.ones_like(self.spearman_corr, dtype=bool))\n",
        "        sns.heatmap(self.spearman_corr, mask=mask, annot=False, fmt='.2f',\n",
        "                   cmap='viridis', center=0, square=True, linewidths=0.5,\n",
        "                   cbar_kws={\"shrink\": 0.8}, ax=ax2, vmin=-1, vmax=1)\n",
        "        ax2.set_title('Spearman Correlation Matrix (Monotonic Relationships)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('spearman_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"✓ Saved: spearman_correlation_heatmap.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 3: Correlation Difference\n",
        "        corr_diff = np.abs(self.pearson_corr - self.spearman_corr)\n",
        "        fig3, ax3 = plt.subplots(figsize=(20, 18))\n",
        "        mask = np.triu(np.ones_like(corr_diff, dtype=bool))\n",
        "        sns.heatmap(corr_diff, mask=mask, annot=False, cmap='Reds',\n",
        "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax3)\n",
        "        ax3.set_title('|Pearson - Spearman| Difference (Non-Linear Indicator)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('correlation_difference.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"✓ Saved: correlation_difference.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 4: Top correlations bar plot\n",
        "        if hasattr(self, 'high_corr_df') and len(self.high_corr_df) > 0:\n",
        "            fig4, ax4 = plt.subplots(figsize=(12, 8))\n",
        "            top_pairs = self.high_corr_df.head(15).copy()\n",
        "            top_pairs['Pair'] = top_pairs['Feature_1'] + '\\n& ' + top_pairs['Feature_2']\n",
        "\n",
        "            colors = ['red' if x > 0.95 else 'orange' for x in top_pairs['Correlation']]\n",
        "            ax4.barh(range(len(top_pairs)), top_pairs['Correlation'], color=colors)\n",
        "            ax4.set_yticks(range(len(top_pairs)))\n",
        "            ax4.set_yticklabels(top_pairs['Pair'], fontsize=9)\n",
        "            ax4.set_xlabel('Absolute Correlation', fontsize=12)\n",
        "            ax4.set_title('Top 15 Highly Correlated Feature Pairs',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax4.axvline(x=0.95, color='red', linestyle='--', label='Critical threshold')\n",
        "            ax4.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('high_correlation_pairs.png', dpi=300, bbox_inches='tight')\n",
        "            print(\"✓ Saved: high_correlation_pairs.png\")\n",
        "            plt.close()\n",
        "\n",
        "        # Figure 5: VIF scores\n",
        "        if hasattr(self, 'vif_df'):\n",
        "            fig5, ax5 = plt.subplots(figsize=(12, 10))\n",
        "            vif_plot = self.vif_df.dropna().head(20)\n",
        "            colors = ['red' if x > 10 else 'orange' if x > 5 else 'green'\n",
        "                     for x in vif_plot['VIF']]\n",
        "\n",
        "            ax5.barh(range(len(vif_plot)), vif_plot['VIF'], color=colors)\n",
        "            ax5.set_yticks(range(len(vif_plot)))\n",
        "            ax5.set_yticklabels(vif_plot['Feature'], fontsize=10)\n",
        "            ax5.set_xlabel('VIF Score', fontsize=12)\n",
        "            ax5.set_title('Variance Inflation Factor (Top 20 Features)',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax5.axvline(x=5, color='orange', linestyle='--', label='Moderate (VIF=5)')\n",
        "            ax5.axvline(x=10, color='red', linestyle='--', label='High (VIF=10)')\n",
        "            ax5.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('vif_scores.png', dpi=300, bbox_inches='tight')\n",
        "            print(\"✓ Saved: vif_scores.png\")\n",
        "            plt.close()\n",
        "\n",
        "    def feature_engineering_recommendations(self):\n",
        "        \"\"\"Provide feature engineering recommendations\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"6. FEATURE ENGINEERING RECOMMENDATIONS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(\"\\n🔧 TIME-SERIES SPECIFIC FEATURES:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        print(\"\\n1️⃣  LAG FEATURES (1-week forecasting horizon):\")\n",
        "        operational_features = [col for col in self.numeric_cols\n",
        "                               if any(x in col for x in ['inj', 'prod', 'whp', 'temp', 'flow'])]\n",
        "        print(\"   Key operational variables to lag:\")\n",
        "        for feat in operational_features[:8]:\n",
        "            print(f\"   • {feat}_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\")\n",
        "\n",
        "        print(\"\\n2️⃣  ROLLING STATISTICS:\")\n",
        "        print(\"   • 24-hour rolling mean/std (daily patterns)\")\n",
        "        print(\"   • 72-hour rolling mean/std (3-day trends)\")\n",
        "        print(\"   • 168-hour rolling mean/std (weekly cycles)\")\n",
        "        print(\"   • Apply to: flow, pressure, temperature, energy metrics\")\n",
        "\n",
        "        print(\"\\n3️⃣  RATE OF CHANGE:\")\n",
        "        print(\"   • First differences: flow_rate_change, pressure_rate_change\")\n",
        "        print(\"   • Acceleration: second differences for critical variables\")\n",
        "\n",
        "        print(\"\\n4️⃣  CUMULATIVE FIELD HANDLING:\")\n",
        "        cumulative = [col for col in self.numeric_cols if 'cum_' in col]\n",
        "        if cumulative:\n",
        "            print(f\"   • Found cumulative features: {cumulative}\")\n",
        "            print(\"   → Convert to incremental (differencing)\")\n",
        "            print(\"   → Or use both raw + differenced with regularization\")\n",
        "\n",
        "        print(\"\\n5️⃣  INTERACTION FEATURES:\")\n",
        "        print(\"   • inj_flow × inj_whp (injection power proxy)\")\n",
        "        print(\"   • prod_temp / prod_flow (temperature efficiency)\")\n",
        "        print(\"   • inj_energy / cooling_energy (energy ratio)\")\n",
        "\n",
        "        print(\"\\n6️⃣  TEMPORAL FEATURES:\")\n",
        "        print(\"   • hour_of_day, day_of_week (cyclical encoding)\")\n",
        "        print(\"   • is_weekend, is_night_shift\")\n",
        "        print(\"   • time_since_last_seismic_event\")\n",
        "\n",
        "    def generate_final_report(self):\n",
        "        \"\"\"Generate comprehensive final report\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"7. FINAL RECOMMENDATIONS & PIPELINE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(\"\\n📋 FEATURE ELIMINATION STRATEGY:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Cumulative features\n",
        "        cumulative = [col for col in self.numeric_cols if 'cum_' in col]\n",
        "        if cumulative:\n",
        "            print(f\"\\n❌ REMOVE (Redundant Cumulative): {len(cumulative)} features\")\n",
        "            for feat in cumulative:\n",
        "                print(f\"   • {feat} → Use differenced version instead\")\n",
        "\n",
        "        # High VIF features\n",
        "        if hasattr(self, 'vif_df'):\n",
        "            high_vif = self.vif_df[self.vif_df['VIF'] > 10]['Feature'].tolist()\n",
        "            if high_vif:\n",
        "                print(f\"\\n⚠️  REVIEW (High VIF): {len(high_vif)} features\")\n",
        "                for feat in high_vif[:5]:\n",
        "                    print(f\"   • {feat}\")\n",
        "\n",
        "        # Low correlation with target\n",
        "        print(\"\\n💡 PRIORITIZE (High target correlation):\")\n",
        "        print(\"   → Keep top 15-20 features with strongest seismic correlation\")\n",
        "        print(\"   → Add their lagged and rolling versions\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SUGGESTED ML PIPELINE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        pipeline_steps = \"\"\"\n",
        "1. DATA PREPROCESSING:\n",
        "   ✓ Handle missing values (forward-fill operational, interpolate seismic)\n",
        "   ✓ Remove/transform high VIF features\n",
        "   ✓ Difference cumulative features\n",
        "\n",
        "2. FEATURE ENGINEERING:\n",
        "   ✓ Create lag features (1-168 hours)\n",
        "   ✓ Compute rolling statistics (24h, 72h, 168h windows)\n",
        "   ✓ Add interaction terms\n",
        "   ✓ Encode temporal features (cyclical)\n",
        "\n",
        "3. FEATURE SELECTION:\n",
        "   ✓ Recursive Feature Elimination (RFE)\n",
        "   ✓ SHAP value analysis (post-model)\n",
        "   ✓ Mutual Information scores\n",
        "   ✓ Domain knowledge filtering\n",
        "\n",
        "4. CROSS-VALIDATION:\n",
        "   ✓ Time-series split (NO shuffle!)\n",
        "   ✓ Respect 1-week forecast horizon\n",
        "   ✓ Walk-forward validation\n",
        "\n",
        "5. MODEL TRAINING:\n",
        "   ✓ Start with: XGBoost, LightGBM, LSTM\n",
        "   ✓ Handle class imbalance (seismic events are rare)\n",
        "   ✓ Optimize for: F1-score, Precision-Recall AUC\n",
        "\"\"\"\n",
        "        print(pipeline_steps)\n",
        "\n",
        "        print(\"\\n⚠️  CRITICAL EDGE CASES:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(\"1. TEMPORAL LEAKAGE:\")\n",
        "        print(\"   • Never use future information in features\")\n",
        "        print(\"   • Lag target variables properly\")\n",
        "        print(\"   • Use only past rolling windows\")\n",
        "\n",
        "        print(\"\\n2. SEISMIC PLACEHOLDER VALUES:\")\n",
        "        print(\"   • -9999 values indicate no event/data\")\n",
        "        print(\"   • Don't impute naively — may indicate information\")\n",
        "        print(\"   • Consider as separate category or zero\")\n",
        "\n",
        "        print(\"\\n3. NON-STATIONARITY:\")\n",
        "        print(\"   • Geothermal systems evolve over time\")\n",
        "        print(\"   • Consider: detrending, adaptive models, online learning\")\n",
        "\n",
        "        print(\"\\n4. RARE EVENT FORECASTING:\")\n",
        "        print(\"   • Seismic events are sparse → class imbalance\")\n",
        "        print(\"   • Use: SMOTE, class weights, focal loss\")\n",
        "        print(\"   • Evaluate on: Precision-Recall, not just accuracy\")\n",
        "\n",
        "        print(\"\\n5. MULTIPLE TARGETS:\")\n",
        "        print(\"   • has_event (binary), magnitude (continuous), rate (count)\")\n",
        "        print(\"   • Consider multi-task learning or ensemble\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ANALYSIS COMPLETE ✓\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\\nNext Steps:\")\n",
        "        print(\"1. Review correlation heatmaps (saved as .png files)\")\n",
        "        print(\"2. Implement feature engineering pipeline\")\n",
        "        print(\"3. Train baseline model with selected features\")\n",
        "        print(\"4. Iterate with SHAP analysis for feature refinement\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Initialize analysis\n",
        "    # Replace 'your_data.csv' with actual file path\n",
        "    analyzer = GeothermalFeatureAnalysis('/content/drive/MyDrive/operational_data_final_11 (1).csv')\n",
        "\n",
        "    # Run analysis pipeline\n",
        "    quality_report = analyzer.data_quality_report()\n",
        "    analyzer.compute_correlations()\n",
        "    high_corr = analyzer.identify_multicollinearity(threshold=0.8)\n",
        "    vif_results = analyzer.compute_vif()\n",
        "\n",
        "    # Target correlation analysis for each seismic variable\n",
        "    for target in analyzer.target_seismic:\n",
        "        if target in analyzer.numeric_cols:\n",
        "            analyzer.target_correlation_ranking(target=target)\n",
        "\n",
        "    # Generate visualizations\n",
        "    analyzer.visualize_correlations()\n",
        "\n",
        "    # Feature engineering recommendations\n",
        "    analyzer.feature_engineering_recommendations()\n",
        "\n",
        "    # Final report\n",
        "    analyzer.generate_final_report()\n",
        "\n",
        "    # Save results\n",
        "    if hasattr(analyzer, 'high_corr_df'):\n",
        "        analyzer.high_corr_df.to_csv('high_correlations.csv', index=False)\n",
        "        print(\"✓ Saved: high_correlations.csv\")\n",
        "\n",
        "    if hasattr(analyzer, 'vif_df'):\n",
        "        analyzer.vif_df.to_csv('vif_scores.csv', index=False)\n",
        "        print(\"✓ Saved: vif_scores.csv\")\n",
        "\n",
        "    print(\"\\n📊 All results saved successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH2BYhDRfAF-",
        "outputId": "ecba7d71-086f-4489-c64b-aff31fc1c7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "GEOTHERMAL TIME-SERIES FEATURE CORRELATION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "✓ Loaded dataset: 695,625 rows × 30 columns\n",
            "✓ Identified 24 numeric features\n",
            "✓ Seismic target variables: ['has_event', 'magnitude', 'hourly_seismicity_rate']\n",
            "\n",
            "================================================================================\n",
            "1. DATA QUALITY REPORT\n",
            "================================================================================\n",
            "\n",
            "📊 Missing Data Summary:\n",
            "                        Missing_Count  Missing_Pct    Dtype\n",
            "distance_to_fault              689675        99.14  float64\n",
            "hourly_seismicity_rate         689675        99.14  float64\n",
            "\n",
            "⚠️  Features with >20% missing data: ['distance_to_fault', 'hourly_seismicity_rate']\n",
            "   → Consider imputation or removal\n",
            "\n",
            "================================================================================\n",
            "2. CORRELATION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "📈 Computing Pearson correlation (linear relationships)...\n",
            "📈 Computing Spearman correlation (monotonic relationships)...\n",
            "✓ Correlation matrices computed: (24, 24)\n",
            "\n",
            "🔍 Identifying multicollinearity (|r| > 0.8)...\n",
            "\n",
            "⚠️  Found 43 feature pairs with |r| > 0.8:\n",
            "     Feature_1           Feature_2  Correlation\n",
            "40  cum_volume  cum_cooling_energy     0.996042\n",
            "38      volume      cooling_energy     0.989139\n",
            "24    prod_whp        is_producing     0.984406\n",
            "6     inj_temp           prod_temp     0.970292\n",
            "26    inj_flow              volume     0.963793\n",
            "33      volume          inj_energy     0.954425\n",
            "35    inj_flow      cooling_energy     0.951976\n",
            "12   prod_temp            prod_whp     0.951614\n",
            "42  hedh_thpwr    heat_exch_energy     0.950040\n",
            "30    inj_flow          inj_energy     0.949584\n",
            "5      inj_whp           prod_temp     0.947086\n",
            "23   prod_temp        is_producing     0.945476\n",
            "14     inj_whp           prod_flow     0.938302\n",
            "39  inj_energy      cooling_energy     0.935606\n",
            "10    inj_temp            prod_whp     0.932145\n",
            "\n",
            "📋 Features involved in high correlations: 16\n",
            "   - cum_inj_energy\n",
            "   - inj_flow\n",
            "   - prod_flow\n",
            "   - prod_temp\n",
            "   - cum_volume\n",
            "   - inj_ap\n",
            "   - cum_cooling_energy\n",
            "   - inj_whp\n",
            "   - volume\n",
            "   - cooling_energy\n",
            "\n",
            "================================================================================\n",
            "3. VIF ANALYSIS (Variance Inflation Factor)\n",
            "================================================================================\n",
            "\n",
            "📊 Using 5,950 complete cases for VIF calculation...\n",
            "\n",
            "📊 VIF Results (Top 15):\n",
            "           Feature          VIF\n",
            "          inj_flow          inf\n",
            "            volume          inf\n",
            "  heat_exch_energy 4.739102e+07\n",
            "        hedh_thpwr 4.738883e+07\n",
            "        cum_volume 1.244911e+03\n",
            "cum_cooling_energy 6.163577e+02\n",
            "    cum_inj_energy 5.132502e+02\n",
            "        inj_energy 4.960832e+02\n",
            "    cooling_energy 3.144147e+02\n",
            "           inj_whp 1.657166e+02\n",
            "      is_producing 1.137984e+02\n",
            "         prod_temp 3.553167e+01\n",
            "          inj_temp 3.486264e+01\n",
            "          prod_whp 2.248990e+01\n",
            "            inj_ap 1.580961e+01\n",
            "\n",
            "📖 VIF Interpretation:\n",
            "   VIF < 5   : Low multicollinearity ✓\n",
            "   5 < VIF < 10 : Moderate multicollinearity ⚠️\n",
            "   VIF > 10  : High multicollinearity ❌ (consider removal)\n",
            "\n",
            "⚠️  Features with VIF > 10: ['inj_flow', 'volume', 'heat_exch_energy', 'hedh_thpwr', 'cum_volume', 'cum_cooling_energy', 'cum_inj_energy', 'inj_energy', 'cooling_energy', 'inj_whp', 'is_producing', 'prod_temp', 'inj_temp', 'prod_whp', 'inj_ap', 'prod_flow']\n",
            "\n",
            "================================================================================\n",
            "4. TARGET CORRELATION RANKING: has_event\n",
            "================================================================================\n",
            "\n",
            "📊 Top 20 Features Correlated with Target:\n",
            "             Feature  Pearson_Corr  Spearman_Corr  Avg_Abs_Corr\n",
            "             pgv_max      0.196223       0.564775      0.380499\n",
            "           magnitude      0.224447       0.426464      0.325456\n",
            "          inj_energy      0.231196       0.121916      0.176556\n",
            "           prod_flow      0.193206       0.120173      0.156689\n",
            "            inj_flow      0.183208       0.120933      0.152070\n",
            "              volume      0.175792       0.120805      0.148299\n",
            "      cooling_energy      0.171626       0.119763      0.145695\n",
            "             inj_whp      0.144405       0.121807      0.133106\n",
            "           prod_temp      0.114264       0.119464      0.116864\n",
            "            inj_temp      0.090893       0.084341      0.087617\n",
            "      cum_inj_energy      0.109229       0.054227      0.081728\n",
            "        is_producing      0.079829       0.079829      0.079829\n",
            "            prod_whp      0.075335       0.066605      0.070970\n",
            "          cum_volume      0.084637       0.040523      0.062580\n",
            "              inj_ap      0.071602       0.052541      0.062071\n",
            "  cum_cooling_energy      0.073851       0.038929      0.056390\n",
            "cum_heat_exch_energy      0.040037       0.051718      0.045877\n",
            "          basin_flow      0.033286       0.036900      0.035093\n",
            "          hedh_thpwr      0.034154       0.034644      0.034399\n",
            "    heat_exch_energy      0.032839       0.034645      0.033742\n",
            "\n",
            "================================================================================\n",
            "4. TARGET CORRELATION RANKING: magnitude\n",
            "================================================================================\n",
            "\n",
            "📊 Top 20 Features Correlated with Target:\n",
            "               Feature  Pearson_Corr  Spearman_Corr  Avg_Abs_Corr\n",
            "               pgv_max      0.747228       0.757826      0.752527\n",
            "hourly_seismicity_rate      0.631612       0.547685      0.589649\n",
            "             has_event      0.224447       0.426464      0.325456\n",
            "            inj_energy      0.046707       0.045969      0.046338\n",
            "        cooling_energy      0.036692       0.046164      0.041428\n",
            "              inj_flow      0.036327       0.046055      0.041191\n",
            "             prod_flow      0.037203       0.044278      0.040741\n",
            "                volume      0.034844       0.046005      0.040424\n",
            "               inj_whp      0.029246       0.047309      0.038278\n",
            "             prod_temp      0.023124       0.046995      0.035059\n",
            "          is_producing      0.015032       0.030149      0.022590\n",
            "              inj_temp      0.014236       0.025224      0.019730\n",
            "              prod_whp      0.013368       0.020598      0.016983\n",
            "     distance_to_fault      0.000006       0.024879      0.012443\n",
            "        cum_inj_energy      0.017867       0.005202      0.011535\n",
            "  cum_heat_exch_energy      0.005718       0.015261      0.010489\n",
            "            basin_flow      0.007716       0.008945      0.008330\n",
            "              gt03_whp      0.010189       0.005287      0.007738\n",
            "            hedh_thpwr      0.007071       0.007871      0.007471\n",
            "      heat_exch_energy      0.006808       0.007872      0.007340\n",
            "\n",
            "================================================================================\n",
            "4. TARGET CORRELATION RANKING: hourly_seismicity_rate\n",
            "================================================================================\n",
            "\n",
            "📊 Top 20 Features Correlated with Target:\n",
            "             Feature  Pearson_Corr  Spearman_Corr  Avg_Abs_Corr\n",
            "           magnitude      0.631612       0.547685      0.589649\n",
            "             pgv_max      0.452842       0.529363      0.491103\n",
            "cum_heat_exch_energy      0.097125       0.134268      0.115697\n",
            "              inj_ap      0.107549       0.118367      0.112958\n",
            "            inj_temp      0.056412       0.112520      0.084466\n",
            "           prod_temp      0.020886       0.104422      0.062654\n",
            "      cooling_energy      0.049212       0.070988      0.060100\n",
            "  cum_cooling_energy      0.057060       0.054945      0.056002\n",
            "            gt03_whp      0.030688       0.078887      0.054787\n",
            "      cum_inj_energy      0.051881       0.040738      0.046310\n",
            "          cum_volume      0.050841       0.038603      0.044722\n",
            "          basin_flow      0.019344       0.068592      0.043968\n",
            "          hedh_thpwr      0.010108       0.068592      0.039350\n",
            "    heat_exch_energy      0.010092       0.068592      0.039342\n",
            "          inj_energy      0.049392       0.029239      0.039316\n",
            "            prod_whp      0.019067       0.055762      0.037414\n",
            "             inj_whp      0.040468       0.033154      0.036811\n",
            "              volume      0.036395       0.023473      0.029934\n",
            "            inj_flow      0.036395       0.023472      0.029933\n",
            "           prod_flow      0.029783       0.001853      0.015818\n",
            "\n",
            "================================================================================\n",
            "5. GENERATING VISUALIZATIONS\n",
            "================================================================================\n",
            "✓ Saved: pearson_correlation_heatmap.png\n",
            "✓ Saved: spearman_correlation_heatmap.png\n",
            "✓ Saved: correlation_difference.png\n",
            "✓ Saved: high_correlation_pairs.png\n",
            "✓ Saved: vif_scores.png\n",
            "\n",
            "================================================================================\n",
            "6. FEATURE ENGINEERING RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "🔧 TIME-SERIES SPECIFIC FEATURES:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1️⃣  LAG FEATURES (1-week forecasting horizon):\n",
            "   Key operational variables to lag:\n",
            "   • inj_flow_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • inj_whp_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • inj_temp_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • inj_ap_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • prod_temp_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • prod_whp_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • gt03_whp_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "   • basin_flow_lag_[1h, 3h, 6h, 12h, 24h, 72h, 168h]\n",
            "\n",
            "2️⃣  ROLLING STATISTICS:\n",
            "   • 24-hour rolling mean/std (daily patterns)\n",
            "   • 72-hour rolling mean/std (3-day trends)\n",
            "   • 168-hour rolling mean/std (weekly cycles)\n",
            "   • Apply to: flow, pressure, temperature, energy metrics\n",
            "\n",
            "3️⃣  RATE OF CHANGE:\n",
            "   • First differences: flow_rate_change, pressure_rate_change\n",
            "   • Acceleration: second differences for critical variables\n",
            "\n",
            "4️⃣  CUMULATIVE FIELD HANDLING:\n",
            "   • Found cumulative features: ['cum_volume', 'cum_inj_energy', 'cum_cooling_energy', 'cum_heat_exch_energy']\n",
            "   → Convert to incremental (differencing)\n",
            "   → Or use both raw + differenced with regularization\n",
            "\n",
            "5️⃣  INTERACTION FEATURES:\n",
            "   • inj_flow × inj_whp (injection power proxy)\n",
            "   • prod_temp / prod_flow (temperature efficiency)\n",
            "   • inj_energy / cooling_energy (energy ratio)\n",
            "\n",
            "6️⃣  TEMPORAL FEATURES:\n",
            "   • hour_of_day, day_of_week (cyclical encoding)\n",
            "   • is_weekend, is_night_shift\n",
            "   • time_since_last_seismic_event\n",
            "\n",
            "================================================================================\n",
            "7. FINAL RECOMMENDATIONS & PIPELINE\n",
            "================================================================================\n",
            "\n",
            "📋 FEATURE ELIMINATION STRATEGY:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "❌ REMOVE (Redundant Cumulative): 4 features\n",
            "   • cum_volume → Use differenced version instead\n",
            "   • cum_inj_energy → Use differenced version instead\n",
            "   • cum_cooling_energy → Use differenced version instead\n",
            "   • cum_heat_exch_energy → Use differenced version instead\n",
            "\n",
            "⚠️  REVIEW (High VIF): 16 features\n",
            "   • inj_flow\n",
            "   • volume\n",
            "   • heat_exch_energy\n",
            "   • hedh_thpwr\n",
            "   • cum_volume\n",
            "\n",
            "💡 PRIORITIZE (High target correlation):\n",
            "   → Keep top 15-20 features with strongest seismic correlation\n",
            "   → Add their lagged and rolling versions\n",
            "\n",
            "================================================================================\n",
            "SUGGESTED ML PIPELINE\n",
            "================================================================================\n",
            "\n",
            "1. DATA PREPROCESSING:\n",
            "   ✓ Handle missing values (forward-fill operational, interpolate seismic)\n",
            "   ✓ Remove/transform high VIF features\n",
            "   ✓ Difference cumulative features\n",
            "   \n",
            "2. FEATURE ENGINEERING:\n",
            "   ✓ Create lag features (1-168 hours)\n",
            "   ✓ Compute rolling statistics (24h, 72h, 168h windows)\n",
            "   ✓ Add interaction terms\n",
            "   ✓ Encode temporal features (cyclical)\n",
            "   \n",
            "3. FEATURE SELECTION:\n",
            "   ✓ Recursive Feature Elimination (RFE)\n",
            "   ✓ SHAP value analysis (post-model)\n",
            "   ✓ Mutual Information scores\n",
            "   ✓ Domain knowledge filtering\n",
            "   \n",
            "4. CROSS-VALIDATION:\n",
            "   ✓ Time-series split (NO shuffle!)\n",
            "   ✓ Respect 1-week forecast horizon\n",
            "   ✓ Walk-forward validation\n",
            "   \n",
            "5. MODEL TRAINING:\n",
            "   ✓ Start with: XGBoost, LightGBM, LSTM\n",
            "   ✓ Handle class imbalance (seismic events are rare)\n",
            "   ✓ Optimize for: F1-score, Precision-Recall AUC\n",
            "\n",
            "\n",
            "⚠️  CRITICAL EDGE CASES:\n",
            "--------------------------------------------------------------------------------\n",
            "1. TEMPORAL LEAKAGE:\n",
            "   • Never use future information in features\n",
            "   • Lag target variables properly\n",
            "   • Use only past rolling windows\n",
            "\n",
            "2. SEISMIC PLACEHOLDER VALUES:\n",
            "   • -9999 values indicate no event/data\n",
            "   • Don't impute naively — may indicate information\n",
            "   • Consider as separate category or zero\n",
            "\n",
            "3. NON-STATIONARITY:\n",
            "   • Geothermal systems evolve over time\n",
            "   • Consider: detrending, adaptive models, online learning\n",
            "\n",
            "4. RARE EVENT FORECASTING:\n",
            "   • Seismic events are sparse → class imbalance\n",
            "   • Use: SMOTE, class weights, focal loss\n",
            "   • Evaluate on: Precision-Recall, not just accuracy\n",
            "\n",
            "5. MULTIPLE TARGETS:\n",
            "   • has_event (binary), magnitude (continuous), rate (count)\n",
            "   • Consider multi-task learning or ensemble\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE ✓\n",
            "================================================================================\n",
            "\n",
            "Next Steps:\n",
            "1. Review correlation heatmaps (saved as .png files)\n",
            "2. Implement feature engineering pipeline\n",
            "3. Train baseline model with selected features\n",
            "4. Iterate with SHAP analysis for feature refinement\n",
            "\n",
            "\n",
            "✓ Saved: high_correlations.csv\n",
            "✓ Saved: vif_scores.csv\n",
            "\n",
            "📊 All results saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier, Pool\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obbLvMmrw_Am",
        "outputId": "24c55a7c-9e1d-4d52-9f5a-1b753e127879"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/SoilDataset/operational_seismic_linear_decay121.csv\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (roc_auc_score, classification_report,\n",
        "                             confusion_matrix, precision_recall_curve,\n",
        "                             average_precision_score)\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = Path('/content/drive/MyDrive/SoilDataset/operational_seismic_linear_decay121.csv')\n",
        "print(f\"Loading data from: {file_path}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\"\\nDataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\\n\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"Column Overview:\")\n",
        "print(\"-\" * 80)\n",
        "print(df.dtypes.to_string())\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Identify key columns\n",
        "time_cols = [c for c in df.columns if any(k in c.lower() for k in ['time', 'date', 'timestamp'])]\n",
        "event_cols = [c for c in df.columns if any(k in c.lower() for k in ['quake', 'event', 'earthquake', 'seismic', 'target', 'label'])]\n",
        "mag_cols = [c for c in df.columns if any(k in c.lower() for k in ['mag', 'magnitude', 'ml', 'richter'])]\n",
        "\n",
        "print(\"\\n🔍 COLUMN DETECTION:\")\n",
        "print(f\"  Time columns: {time_cols}\")\n",
        "print(f\"  Event columns: {event_cols}\")\n",
        "print(f\"  Magnitude columns: {mag_cols}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create target variable\n",
        "def create_earthquake_target(df, time_col=None, event_col=None, mag_col=None,\n",
        "                            window_days=2, mag_threshold=3.0):\n",
        "    \"\"\"\n",
        "    Create binary target: 1 if earthquake occurs within window_days, 0 otherwise\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Strategy 1: Use existing binary target if available\n",
        "    for col in event_cols:\n",
        "        if col in df.columns:\n",
        "            unique_vals = df[col].dropna().unique()\n",
        "            if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, '0', '1', True, False}):\n",
        "                print(f\"\\n✓ Using existing binary target column: '{col}'\")\n",
        "                df['target'] = df[col].astype(int)\n",
        "                return df\n",
        "\n",
        "    # Strategy 2: Time-based window with magnitude threshold\n",
        "    if time_col and mag_col:\n",
        "        print(f\"\\n✓ Creating target from time window analysis...\")\n",
        "        print(f\"  Time column: {time_col}\")\n",
        "        print(f\"  Magnitude column: {mag_col}\")\n",
        "        print(f\"  Window: {window_days} days\")\n",
        "        print(f\"  Magnitude threshold: {mag_threshold}\")\n",
        "\n",
        "        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "        df = df.sort_values(time_col).reset_index(drop=True)\n",
        "        df['target'] = 0\n",
        "\n",
        "        times = df[time_col].values\n",
        "        mags = df[mag_col].values\n",
        "\n",
        "        for i in range(len(df)):\n",
        "            if pd.isna(times[i]):\n",
        "                continue\n",
        "            cutoff = times[i] + np.timedelta64(window_days, 'D')\n",
        "            mask = (times > times[i]) & (times <= cutoff) & (mags >= mag_threshold) & (~np.isnan(mags))\n",
        "            if mask.any():\n",
        "                df.at[i, 'target'] = 1\n",
        "\n",
        "        print(f\"  ✓ Target created based on {window_days}-day window\")\n",
        "        return df\n",
        "\n",
        "    # Strategy 3: Binarize event column\n",
        "    if event_col:\n",
        "        print(f\"\\n✓ Binarizing event column: '{event_col}'\")\n",
        "        df['target'] = (df[event_col].fillna(0) > 0).astype(int)\n",
        "        return df\n",
        "\n",
        "    raise ValueError(\"Could not create target variable. Please check your data columns.\")\n",
        "\n",
        "# Determine which columns to use\n",
        "tcol = time_cols[0] if time_cols else None\n",
        "mcol = mag_cols[0] if mag_cols else None\n",
        "ecol = event_cols[0] if event_cols else None\n",
        "\n",
        "# Create target\n",
        "df = create_earthquake_target(df, time_col=tcol, event_col=ecol, mag_col=mcol,\n",
        "                              window_days=2, mag_threshold=3.0)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION:\")\n",
        "print(\"-\" * 80)\n",
        "target_counts = df['target'].value_counts()\n",
        "print(f\"  Class 0 (No earthquake): {target_counts.get(0, 0):,} ({target_counts.get(0, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"  Class 1 (Earthquake):    {target_counts.get(1, 0):,} ({target_counts.get(1, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"  Imbalance ratio: {target_counts.get(0, 0)/max(target_counts.get(1, 1), 1):.2f}:1\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare features\n",
        "drop_cols = list(set(time_cols + event_cols + [c for c in ['target'] if c in df.columns]))\n",
        "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df['target'].astype(int)\n",
        "\n",
        "print(f\"\\n📊 FEATURE ENGINEERING:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Dropped columns: {drop_cols}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\n🔧 DATA PREPROCESSING:\")\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "print(f\"  Numeric features: {len(numeric_cols)}\")\n",
        "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if X[col].isna().sum() > 0:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "for col in categorical_cols:\n",
        "    X[col] = X[col].astype(str).fillna('missing')\n",
        "\n",
        "print(\"  ✓ Missing values handled\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n📦 TRAIN/TEST SPLIT:\")\n",
        "print(f\"  Train: {X_train.shape[0]:,} samples\")\n",
        "print(f\"  Test:  {X_test.shape[0]:,} samples\")\n",
        "print(f\"  Train positive rate: {y_train.mean():.4f}\")\n",
        "print(f\"  Test positive rate:  {y_test.mean():.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Identify categorical features for CatBoost\n",
        "cat_features = [i for i, col in enumerate(X.columns) if col in categorical_cols]\n",
        "\n",
        "# Train CatBoost model\n",
        "print(\"\\n🤖 TRAINING CATBOOST MODEL:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    eval_metric='AUC',\n",
        "    early_stopping_rounds=50,\n",
        "    auto_class_weights='Balanced'  # Handle class imbalance\n",
        ")\n",
        "\n",
        "# Create Pool objects for better performance\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
        "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
        "\n",
        "model.fit(\n",
        "    train_pool,\n",
        "    eval_set=test_pool,\n",
        "    use_best_model=True,\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "print(\"✓ Model training complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Predictions\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"\\n📈 MODEL PERFORMANCE:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "ap_score = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"  ROC AUC Score: {auc_score:.4f}\")\n",
        "print(f\"  Average Precision Score: {ap_score:.4f}\")\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred,\n",
        "                          target_names=['No Earthquake', 'Earthquake'],\n",
        "                          zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"  True Negatives:  {cm[0, 0]:,}\")\n",
        "print(f\"  False Positives: {cm[0, 1]:,}\")\n",
        "print(f\"  False Negatives: {cm[1, 0]:,}\")\n",
        "print(f\"  True Positives:  {cm[1, 1]:,}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\n🎯 TOP 20 MOST IMPORTANT FEATURES:\")\n",
        "print(\"-\" * 80)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(20).iterrows():\n",
        "    print(f\"  {row['feature']:40s} {row['importance']:8.2f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save processed data\n",
        "out_path = '/content/drive/MyDrive/SoilDataset/earthquake_prediction_processed.csv'\n",
        "df_processed = pd.concat([X, y.rename('target')], axis=1)\n",
        "df_processed.to_csv(out_path, index=False)\n",
        "print(f\"\\n💾 Processed dataset saved to: {out_path}\")\n",
        "\n",
        "# Save model\n",
        "model_path = '/content/drive/MyDrive/SoilDataset/earthquake_catboost_model.cbm'\n",
        "model.save_model(model_path)\n",
        "print(f\"💾 Model saved to: {model_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Summary results\n",
        "results = {\n",
        "    'dataset_shape': df.shape,\n",
        "    'num_features': X.shape[1],\n",
        "    'train_size': len(X_train),\n",
        "    'test_size': len(X_test),\n",
        "    'roc_auc': auc_score,\n",
        "    'avg_precision': ap_score,\n",
        "    'model_iterations': model.tree_count_,\n",
        "    'best_iteration': model.get_best_iteration()\n",
        "}\n",
        "\n",
        "print(\"\\n📋 SUMMARY:\")\n",
        "for key, value in results.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "id": "M5LDfd5TuzgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324a87e6-cc17-4561-c235-14dfe7d86136"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from: /content/drive/MyDrive/SoilDataset/operational_seismic_linear_decay121.csv\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2101758384.py:20: DtypeWarning: Columns (24,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Shape: 696,275 rows × 30 columns\n",
            "\n",
            "Column Overview:\n",
            "--------------------------------------------------------------------------------\n",
            "recorded_at                   object\n",
            "phase                        float64\n",
            "inj_flow                     float64\n",
            "inj_whp                      float64\n",
            "inj_temp                     float64\n",
            "inj_ap                       float64\n",
            "prod_temp                    float64\n",
            "prod_whp                     float64\n",
            "gt03_whp                     float64\n",
            "hedh_thpwr                   float64\n",
            "basin_flow                   float64\n",
            "prod_flow                    float64\n",
            "is_producing                    bool\n",
            "phase_started_at              object\n",
            "phase_production_ended_at     object\n",
            "phase_ended_at                object\n",
            "volume                       float64\n",
            "cum_volume                   float64\n",
            "inj_energy                   float64\n",
            "cum_inj_energy               float64\n",
            "cooling_energy               float64\n",
            "cum_cooling_energy           float64\n",
            "heat_exch_energy             float64\n",
            "cum_heat_exch_energy         float64\n",
            "occurred_at                   object\n",
            "pgv_max                      float64\n",
            "magnitude                    float64\n",
            "hourly_seismicity_rate       float64\n",
            "rounded                       object\n",
            "adjusted                      object\n",
            "\n",
            "================================================================================\n",
            "\n",
            "First few rows:\n",
            "           recorded_at  phase   inj_flow    inj_whp   inj_temp  inj_ap  \\\n",
            "0  2018-11-28 11:20:00    0.0   3.473958  29.742170  18.670664     0.0   \n",
            "1  2018-11-28 11:25:00    0.0  25.510803  29.709708  11.027036     0.0   \n",
            "2  2018-11-28 11:30:00    0.0   1.215181  29.675275  10.577293     0.0   \n",
            "3  2018-11-28 11:35:00    0.0   8.490693  29.644622  10.619303     0.0   \n",
            "4  2018-11-28 11:40:00    0.0   0.057919  29.614783  10.622052     0.0   \n",
            "\n",
            "   prod_temp  prod_whp  gt03_whp  hedh_thpwr  ...  cooling_energy  \\\n",
            "0  22.161097  5.309335       0.0  129.246162  ...        0.000000   \n",
            "1  22.115343  5.307418       0.0  129.227933  ...        0.306809   \n",
            "2  22.089825  5.306586       0.0  129.209704  ...        0.014663   \n",
            "3  22.057563  5.306062       0.0  129.191474  ...        0.102423   \n",
            "4  22.148908  5.306007       0.0  129.173245  ...        0.000699   \n",
            "\n",
            "   cum_cooling_energy  heat_exch_energy cum_heat_exch_energy occurred_at  \\\n",
            "0            0.000000          0.010781            37.625953         NaN   \n",
            "1            0.306809          0.010779            37.620647         NaN   \n",
            "2            0.321472          0.010778            37.615340         NaN   \n",
            "3            0.423895          0.010776            37.610033         NaN   \n",
            "4            0.424594          0.010775            37.604726         NaN   \n",
            "\n",
            "  pgv_max  magnitude  hourly_seismicity_rate  rounded  adjusted  \n",
            "0  -999.0     -999.0                  -999.0      NaN       NaN  \n",
            "1  -999.0     -999.0                  -999.0      NaN       NaN  \n",
            "2  -999.0     -999.0                  -999.0      NaN       NaN  \n",
            "3  -999.0     -999.0                  -999.0      NaN       NaN  \n",
            "4  -999.0     -999.0                  -999.0      NaN       NaN  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "\n",
            "================================================================================\n",
            "\n",
            "🔍 COLUMN DETECTION:\n",
            "  Time columns: []\n",
            "  Event columns: ['hourly_seismicity_rate']\n",
            "  Magnitude columns: ['magnitude']\n",
            "================================================================================\n",
            "\n",
            "✓ Binarizing event column: 'hourly_seismicity_rate'\n",
            "\n",
            "================================================================================\n",
            "TARGET VARIABLE DISTRIBUTION:\n",
            "--------------------------------------------------------------------------------\n",
            "  Class 0 (No earthquake): 695,618 (99.91%)\n",
            "  Class 1 (Earthquake):    657 (0.09%)\n",
            "  Imbalance ratio: 1058.78:1\n",
            "================================================================================\n",
            "\n",
            "📊 FEATURE ENGINEERING:\n",
            "  Total features: 29\n",
            "  Dropped columns: ['hourly_seismicity_rate', 'target']\n",
            "================================================================================\n",
            "\n",
            "🔧 DATA PREPROCESSING:\n",
            "  Numeric features: 21\n",
            "  Categorical features: 8\n",
            "  ✓ Missing values handled\n",
            "================================================================================\n",
            "\n",
            "📦 TRAIN/TEST SPLIT:\n",
            "  Train: 557,020 samples\n",
            "  Test:  139,255 samples\n",
            "  Train positive rate: 0.0009\n",
            "  Test positive rate:  0.0009\n",
            "================================================================================\n",
            "\n",
            "🤖 TRAINING CATBOOST MODEL:\n",
            "--------------------------------------------------------------------------------\n",
            "0:\ttest: 0.9960185\tbest: 0.9960185 (0)\ttotal: 2s\tremaining: 33m 15s\n",
            "100:\ttest: 0.9999630\tbest: 0.9999630 (100)\ttotal: 2m 24s\tremaining: 21m 24s\n",
            "200:\ttest: 0.9999780\tbest: 0.9999785 (180)\ttotal: 4m 9s\tremaining: 16m 31s\n",
            "300:\ttest: 0.9999835\tbest: 0.9999835 (299)\ttotal: 5m 49s\tremaining: 13m 32s\n",
            "400:\ttest: 0.9999841\tbest: 0.9999841 (397)\ttotal: 7m 19s\tremaining: 10m 56s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 0.999984088\n",
            "bestIteration = 397\n",
            "\n",
            "Shrink model to first 398 iterations.\n",
            "✓ Model training complete!\n",
            "================================================================================\n",
            "\n",
            "📈 MODEL PERFORMANCE:\n",
            "================================================================================\n",
            "  ROC AUC Score: 1.0000\n",
            "  Average Precision Score: 0.9871\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Earthquake       1.00      1.00      1.00    139124\n",
            "   Earthquake       0.62      0.98      0.76       131\n",
            "\n",
            "     accuracy                           1.00    139255\n",
            "    macro avg       0.81      0.99      0.88    139255\n",
            " weighted avg       1.00      1.00      1.00    139255\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "  True Negatives:  139,046\n",
            "  False Positives: 78\n",
            "  False Negatives: 2\n",
            "  True Positives:  129\n",
            "================================================================================\n",
            "\n",
            "🎯 TOP 20 MOST IMPORTANT FEATURES:\n",
            "--------------------------------------------------------------------------------\n",
            "  inj_temp                                    16.74\n",
            "  phase_started_at                            12.36\n",
            "  cum_cooling_energy                           9.55\n",
            "  inj_whp                                      8.02\n",
            "  cum_inj_energy                               7.37\n",
            "  prod_whp                                     7.21\n",
            "  magnitude                                    5.76\n",
            "  pgv_max                                      4.13\n",
            "  gt03_whp                                     3.93\n",
            "  cum_volume                                   3.01\n",
            "  phase_ended_at                               2.97\n",
            "  prod_temp                                    2.85\n",
            "  inj_ap                                       2.78\n",
            "  phase_production_ended_at                    2.71\n",
            "  rounded                                      1.92\n",
            "  heat_exch_energy                             1.64\n",
            "  inj_energy                                   1.37\n",
            "  prod_flow                                    1.17\n",
            "  hedh_thpwr                                   1.03\n",
            "  basin_flow                                   0.99\n",
            "================================================================================\n",
            "\n",
            "💾 Processed dataset saved to: /content/drive/MyDrive/SoilDataset/earthquake_prediction_processed.csv\n",
            "💾 Model saved to: /content/drive/MyDrive/SoilDataset/earthquake_catboost_model.cbm\n",
            "\n",
            "================================================================================\n",
            "✅ ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📋 SUMMARY:\n",
            "  dataset_shape: (696275, 31)\n",
            "  num_features: 29\n",
            "  train_size: 557020\n",
            "  test_size: 139255\n",
            "  roc_auc: 0.9999840880045282\n",
            "  avg_precision: 0.987085617112416\n",
            "  model_iterations: 398\n",
            "  best_iteration: 397\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import (roc_auc_score, classification_report,\n",
        "                             confusion_matrix, precision_recall_curve,\n",
        "                             average_precision_score, roc_curve)\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🌍 ENHANCED EARTHQUAKE PREDICTION SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "file_path = Path('/content/drive/MyDrive/SoilDataset/operational_seismic_linear_decay121.csv')\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "print(f\"\\n✓ Loaded {len(df):,} records with {df.shape[1]} features\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: ADVANCED DATA CLEANING\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: DATA CLEANING & FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Handle -999.0 sentinel values (common in geophysical data)\n",
        "sentinel_cols = ['pgv_max', 'magnitude', 'hourly_seismicity_rate']\n",
        "for col in sentinel_cols:\n",
        "    if col in df.columns:\n",
        "        mask = df[col] == -999.0\n",
        "        print(f\"  {col}: Replacing {mask.sum():,} sentinel values (-999.0) with NaN\")\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "# Parse datetime columns\n",
        "datetime_cols = ['recorded_at', 'phase_started_at', 'phase_production_ended_at',\n",
        "                 'phase_ended_at', 'occurred_at']\n",
        "for col in datetime_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        print(f\"  ✓ Parsed {col}\")\n",
        "\n",
        "# Sort by time for proper time-series handling\n",
        "df = df.sort_values('recorded_at').reset_index(drop=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: CREATE ENHANCED TARGET VARIABLE\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: CREATING 2-DAY EARTHQUAKE PREDICTION TARGET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create target based on whether an earthquake occurs in next 2 days\n",
        "df['target'] = 0\n",
        "window_hours = 48  # 2 days\n",
        "\n",
        "print(f\"\\n  Strategy: Look ahead {window_hours} hours for earthquakes\")\n",
        "print(f\"  Earthquake definition: magnitude >= 2.0 OR hourly_seismicity_rate > 0\")\n",
        "\n",
        "if 'occurred_at' in df.columns and df['occurred_at'].notna().any():\n",
        "    # Use actual earthquake occurrence times\n",
        "    earthquake_times = df[df['occurred_at'].notna()]['occurred_at'].values\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        current_time = df.loc[i, 'recorded_at']\n",
        "        if pd.isna(current_time):\n",
        "            continue\n",
        "\n",
        "        window_end = current_time + timedelta(hours=window_hours)\n",
        "\n",
        "        # Check if any earthquake occurs in the window\n",
        "        future_quakes = (earthquake_times > current_time) & (earthquake_times <= window_end)\n",
        "        if future_quakes.any():\n",
        "            df.at[i, 'target'] = 1\n",
        "else:\n",
        "    # Fallback: use magnitude and seismicity rate as proxy\n",
        "    for i in range(len(df) - 1):\n",
        "        current_time = df.loc[i, 'recorded_at']\n",
        "        if pd.isna(current_time):\n",
        "            continue\n",
        "\n",
        "        window_end = current_time + timedelta(hours=window_hours)\n",
        "\n",
        "        # Look at future records within window\n",
        "        future_mask = (df['recorded_at'] > current_time) & (df['recorded_at'] <= window_end)\n",
        "        future_data = df[future_mask]\n",
        "\n",
        "        # Check for significant seismic activity\n",
        "        has_earthquake = (\n",
        "            (future_data['magnitude'] >= 2.0).any() |\n",
        "            (future_data['hourly_seismicity_rate'] > 0).any()\n",
        "        )\n",
        "\n",
        "        if has_earthquake:\n",
        "            df.at[i, 'target'] = 1\n",
        "\n",
        "target_dist = df['target'].value_counts()\n",
        "print(f\"\\n  ✓ Target created:\")\n",
        "print(f\"    Class 0 (No earthquake in 48h): {target_dist.get(0, 0):,} ({target_dist.get(0, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"    Class 1 (Earthquake in 48h):    {target_dist.get(1, 0):,} ({target_dist.get(1, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"    Imbalance ratio: {target_dist.get(0, 0)/max(target_dist.get(1, 1), 1):.1f}:1\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: FEATURE ENGINEERING\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: ADVANCED FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract temporal features\n",
        "df['hour'] = df['recorded_at'].dt.hour\n",
        "df['day_of_week'] = df['recorded_at'].dt.dayofweek\n",
        "df['day_of_month'] = df['recorded_at'].dt.day\n",
        "df['month'] = df['recorded_at'].dt.month\n",
        "\n",
        "# Calculate rolling statistics (earthquake precursors)\n",
        "rolling_windows = [6, 12, 24]  # hours\n",
        "for window in rolling_windows:\n",
        "    df[f'inj_temp_roll_mean_{window}h'] = df['inj_temp'].rolling(window, min_periods=1).mean()\n",
        "    df[f'inj_whp_roll_std_{window}h'] = df['inj_whp'].rolling(window, min_periods=1).std()\n",
        "    df[f'prod_flow_roll_max_{window}h'] = df['prod_flow'].rolling(window, min_periods=1).max()\n",
        "\n",
        "# Rate of change features (sudden changes may indicate earthquake risk)\n",
        "df['inj_temp_diff'] = df['inj_temp'].diff()\n",
        "df['inj_whp_diff'] = df['inj_whp'].diff()\n",
        "df['cum_inj_energy_diff'] = df['cum_inj_energy'].diff()\n",
        "\n",
        "# Energy ratios\n",
        "df['cooling_to_inj_ratio'] = df['cooling_energy'] / (df['inj_energy'] + 1e-6)\n",
        "df['heat_exch_efficiency'] = df['heat_exch_energy'] / (df['inj_energy'] + 1e-6)\n",
        "\n",
        "print(f\"  ✓ Created {len([c for c in df.columns if c.endswith(('_diff', '_ratio', 'h', 'efficiency'))])} engineered features\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: PREPARE DATA FOR MODELING\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: DATA PREPARATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define feature sets\n",
        "exclude_cols = ['recorded_at', 'phase_started_at', 'phase_production_ended_at',\n",
        "                'phase_ended_at', 'occurred_at', 'target', 'hourly_seismicity_rate',\n",
        "                'rounded', 'adjusted']  # Remove leaky features\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "X = df[feature_cols].copy()\n",
        "y = df['target'].astype(int)\n",
        "\n",
        "# Handle missing values\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if X[col].isna().sum() > 0:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "for col in categorical_cols:\n",
        "    X[col] = X[col].astype(str).fillna('missing')\n",
        "\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Numeric: {len(numeric_cols)}, Categorical: {len(categorical_cols)}\")\n",
        "\n",
        "# Time-series aware split (use last 20% for testing to avoid look-ahead bias)\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "print(f\"\\n  Train set: {len(X_train):,} samples (first 80% chronologically)\")\n",
        "print(f\"  Test set:  {len(X_test):,} samples (last 20% chronologically)\")\n",
        "print(f\"  Train positive rate: {y_train.mean():.4f}\")\n",
        "print(f\"  Test positive rate:  {y_test.mean():.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: TRAIN OPTIMIZED MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: TRAINING OPTIMIZED CATBOOST MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cat_features = [i for i, col in enumerate(X.columns) if col in categorical_cols]\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=800,\n",
        "    learning_rate=0.03,\n",
        "    depth=8,\n",
        "    l2_leaf_reg=5,\n",
        "    min_data_in_leaf=20,\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    eval_metric='AUC',\n",
        "    early_stopping_rounds=75,\n",
        "    auto_class_weights='Balanced',\n",
        "    task_type='CPU',\n",
        "    bootstrap_type='Bernoulli',\n",
        "    subsample=0.8\n",
        ")\n",
        "\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
        "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
        "\n",
        "model.fit(train_pool, eval_set=test_pool, use_best_model=True, plot=False)\n",
        "\n",
        "print(f\"\\n  ✓ Training complete! Best iteration: {model.get_best_iteration()}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: COMPREHENSIVE EVALUATION\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: MODEL EVALUATION & PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Core metrics\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "ap_score = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\n📊 OVERALL PERFORMANCE:\")\n",
        "print(f\"  ROC AUC Score:        {auc_score:.6f}\")\n",
        "print(f\"  Avg Precision Score:  {ap_score:.6f}\")\n",
        "\n",
        "print(f\"\\n📋 CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_test, y_pred,\n",
        "                          target_names=['No Earthquake', 'Earthquake'],\n",
        "                          zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(f\"📉 CONFUSION MATRIX:\")\n",
        "print(f\"  True Negatives:  {tn:,}  (Correct: no earthquake predicted, none occurred)\")\n",
        "print(f\"  False Positives: {fp:,}  (False alarm: predicted earthquake, none occurred)\")\n",
        "print(f\"  False Negatives: {fn:,}  (MISSED: earthquake occurred but not predicted)\")\n",
        "print(f\"  True Positives:  {tp:,}  (Correct: earthquake predicted and occurred)\")\n",
        "\n",
        "# Calculate business metrics\n",
        "print(f\"\\n💡 OPERATIONAL INSIGHTS:\")\n",
        "print(f\"  Earthquake Detection Rate: {tp/(tp+fn)*100:.1f}% (caught {tp} out of {tp+fn} events)\")\n",
        "print(f\"  False Alarm Rate: {fp/(tn+fp)*100:.2f}% ({fp:,} false alarms)\")\n",
        "print(f\"  Precision (when we predict earthquake): {tp/(tp+fp)*100:.1f}%\")\n",
        "\n",
        "# Find optimal threshold\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "print(f\"\\n🎯 THRESHOLD OPTIMIZATION:\")\n",
        "print(f\"  Default threshold: 0.50\")\n",
        "print(f\"  Optimal F1 threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"  (Maximizes balance between precision and recall)\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\n🔝 TOP 15 PREDICTIVE FEATURES:\")\n",
        "print(\"-\" * 80)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(15).iterrows():\n",
        "    print(f\"  {row['feature']:45s} {row['importance']:8.2f}\")\n",
        "\n",
        "# Save outputs\n",
        "model_path = '/content/drive/MyDrive/SoilDataset/earthquake_optimized_model.cbm'\n",
        "model.save_model(model_path)\n",
        "\n",
        "feature_importance.to_csv('/content/drive/MyDrive/SoilDataset/feature_importance.csv', index=False)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'actual': y_test,\n",
        "    'predicted_proba': y_pred_proba,\n",
        "    'predicted_class': y_pred\n",
        "})\n",
        "results_df.to_csv('/content/drive/MyDrive/SoilDataset/predictions.csv', index=False)\n",
        "\n",
        "print(f\"\\n💾 SAVED:\")\n",
        "print(f\"  Model: {model_path}\")\n",
        "print(f\"  Feature importance: feature_importance.csv\")\n",
        "print(f\"  Predictions: predictions.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ ENHANCED ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final summary\n",
        "summary = {\n",
        "    'Total Records': len(df),\n",
        "    'Features Used': X.shape[1],\n",
        "    'Test Set Size': len(X_test),\n",
        "    'ROC AUC': f\"{auc_score:.6f}\",\n",
        "    'Avg Precision': f\"{ap_score:.4f}\",\n",
        "    'Detection Rate': f\"{tp/(tp+fn)*100:.1f}%\",\n",
        "    'False Alarm Rate': f\"{fp/(tn+fp)*100:.2f}%\",\n",
        "    'Optimal Threshold': f\"{optimal_threshold:.3f}\"\n",
        "}\n",
        "\n",
        "print(\"\\n📊 FINAL SUMMARY:\")\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key:20s}: {value}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lsoViyHvpfo",
        "outputId": "79ee2c6f-1482-4a47-c277-0bbff3ee9905"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🌍 ENHANCED EARTHQUAKE PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "✓ Loaded 696,275 records with 30 features\n",
            "\n",
            "================================================================================\n",
            "STEP 1: DATA CLEANING & FEATURE ENGINEERING\n",
            "================================================================================\n",
            "  pgv_max: Replacing 689,407 sentinel values (-999.0) with NaN\n",
            "  magnitude: Replacing 689,407 sentinel values (-999.0) with NaN\n",
            "  hourly_seismicity_rate: Replacing 689,407 sentinel values (-999.0) with NaN\n",
            "  ✓ Parsed recorded_at\n",
            "  ✓ Parsed phase_started_at\n",
            "  ✓ Parsed phase_production_ended_at\n",
            "  ✓ Parsed phase_ended_at\n",
            "  ✓ Parsed occurred_at\n",
            "\n",
            "================================================================================\n",
            "STEP 2: CREATING 2-DAY EARTHQUAKE PREDICTION TARGET\n",
            "================================================================================\n",
            "\n",
            "  Strategy: Look ahead 48 hours for earthquakes\n",
            "  Earthquake definition: magnitude >= 2.0 OR hourly_seismicity_rate > 0\n",
            "\n",
            "  ✓ Target created:\n",
            "    Class 0 (No earthquake in 48h): 630,022 (90.48%)\n",
            "    Class 1 (Earthquake in 48h):    66,253 (9.52%)\n",
            "    Imbalance ratio: 9.5:1\n",
            "\n",
            "================================================================================\n",
            "STEP 3: ADVANCED FEATURE ENGINEERING\n",
            "================================================================================\n",
            "  ✓ Created 16 engineered features\n",
            "\n",
            "================================================================================\n",
            "STEP 4: DATA PREPARATION\n",
            "================================================================================\n",
            "  Total features: 40\n",
            "  Numeric: 39, Categorical: 1\n",
            "\n",
            "  Train set: 557,020 samples (first 80% chronologically)\n",
            "  Test set:  139,255 samples (last 20% chronologically)\n",
            "  Train positive rate: 0.1148\n",
            "  Test positive rate:  0.0167\n",
            "\n",
            "================================================================================\n",
            "STEP 5: TRAINING OPTIMIZED CATBOOST MODEL\n",
            "================================================================================\n",
            "0:\ttest: 0.5444776\tbest: 0.5444776 (0)\ttotal: 503ms\tremaining: 6m 41s\n",
            "Stopped by overfitting detector  (75 iterations wait)\n",
            "\n",
            "bestTest = 0.6452724102\n",
            "bestIteration = 12\n",
            "\n",
            "Shrink model to first 13 iterations.\n",
            "\n",
            "  ✓ Training complete! Best iteration: 12\n",
            "\n",
            "================================================================================\n",
            "STEP 6: MODEL EVALUATION & PERFORMANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "📊 OVERALL PERFORMANCE:\n",
            "  ROC AUC Score:        0.645272\n",
            "  Avg Precision Score:  0.038038\n",
            "\n",
            "📋 CLASSIFICATION REPORT:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Earthquake       0.98      0.91      0.94    136932\n",
            "   Earthquake       0.00      0.00      0.00      2323\n",
            "\n",
            "     accuracy                           0.89    139255\n",
            "    macro avg       0.49      0.45      0.47    139255\n",
            " weighted avg       0.97      0.89      0.93    139255\n",
            "\n",
            "📉 CONFUSION MATRIX:\n",
            "  True Negatives:  124,116  (Correct: no earthquake predicted, none occurred)\n",
            "  False Positives: 12,816  (False alarm: predicted earthquake, none occurred)\n",
            "  False Negatives: 2,323  (MISSED: earthquake occurred but not predicted)\n",
            "  True Positives:  0  (Correct: earthquake predicted and occurred)\n",
            "\n",
            "💡 OPERATIONAL INSIGHTS:\n",
            "  Earthquake Detection Rate: 0.0% (caught 0 out of 2323 events)\n",
            "  False Alarm Rate: 9.36% (12,816 false alarms)\n",
            "  Precision (when we predict earthquake): 0.0%\n",
            "\n",
            "🎯 THRESHOLD OPTIMIZATION:\n",
            "  Default threshold: 0.50\n",
            "  Optimal F1 threshold: 0.480\n",
            "  (Maximizes balance between precision and recall)\n",
            "\n",
            "🔝 TOP 15 PREDICTIVE FEATURES:\n",
            "--------------------------------------------------------------------------------\n",
            "  day_of_month                                     19.94\n",
            "  month                                            16.51\n",
            "  phase                                            14.93\n",
            "  prod_temp                                        11.74\n",
            "  gt03_whp                                          8.01\n",
            "  inj_ap                                            5.33\n",
            "  cum_inj_energy                                    3.98\n",
            "  cum_volume                                        3.88\n",
            "  day_of_week                                       3.86\n",
            "  inj_whp                                           3.16\n",
            "  cum_cooling_energy                                2.27\n",
            "  prod_whp                                          1.56\n",
            "  inj_temp                                          1.31\n",
            "  inj_temp_roll_mean_24h                            1.14\n",
            "  cooling_to_inj_ratio                              0.78\n",
            "\n",
            "💾 SAVED:\n",
            "  Model: /content/drive/MyDrive/SoilDataset/earthquake_optimized_model.cbm\n",
            "  Feature importance: feature_importance.csv\n",
            "  Predictions: predictions.csv\n",
            "\n",
            "================================================================================\n",
            "✅ ENHANCED ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📊 FINAL SUMMARY:\n",
            "  Total Records       : 696275\n",
            "  Features Used       : 40\n",
            "  Test Set Size       : 139255\n",
            "  ROC AUC             : 0.645272\n",
            "  Avg Precision       : 0.0380\n",
            "  Detection Rate      : 0.0%\n",
            "  False Alarm Rate    : 9.36%\n",
            "  Optimal Threshold   : 0.480\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (roc_auc_score, classification_report,\n",
        "                             confusion_matrix, precision_recall_curve,\n",
        "                             average_precision_score, roc_curve)\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🌍 OPTIMIZED EARTHQUAKE PREDICTION SYSTEM v2\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "file_path = Path('/content/drive/MyDrive/SoilDataset/operational_seismic_linear_decay121.csv')\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "print(f\"\\n✓ Loaded {len(df):,} records with {df.shape[1]} features\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: IMPROVED DATA CLEANING\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: DATA CLEANING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Handle -999.0 sentinel values\n",
        "sentinel_cols = ['pgv_max', 'magnitude', 'hourly_seismicity_rate']\n",
        "for col in sentinel_cols:\n",
        "    if col in df.columns:\n",
        "        mask = df[col] == -999.0\n",
        "        print(f\"  {col}: Replacing {mask.sum():,} sentinel values with NaN\")\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "# Parse datetime\n",
        "datetime_cols = ['recorded_at', 'phase_started_at', 'phase_production_ended_at',\n",
        "                 'phase_ended_at', 'occurred_at']\n",
        "for col in datetime_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "df = df.sort_values('recorded_at').reset_index(drop=True)\n",
        "print(\"  ✓ Data cleaned and sorted by time\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: SMART TARGET CREATION (Keep original approach that worked!)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: TARGET VARIABLE CREATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use the approach from the successful first model\n",
        "# Target = 1 if hourly_seismicity_rate > 0 (actual seismic event detected)\n",
        "df['target'] = ((df['hourly_seismicity_rate'].fillna(0) > 0) |\n",
        "                (df['magnitude'].fillna(0) >= 2.0)).astype(int)\n",
        "\n",
        "target_dist = df['target'].value_counts()\n",
        "print(f\"\\n  Strategy: Binary classification of seismic events\")\n",
        "print(f\"  Definition: magnitude >= 2.0 OR hourly_seismicity_rate > 0\")\n",
        "print(f\"\\n  Class 0 (No earthquake): {target_dist.get(0, 0):,} ({target_dist.get(0, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"  Class 1 (Earthquake):    {target_dist.get(1, 0):,} ({target_dist.get(1, 0)/len(df)*100:.2f}%)\")\n",
        "print(f\"  Imbalance ratio: {target_dist.get(0, 0)/max(target_dist.get(1, 1), 1):.1f}:1\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: ADVANCED FEATURE ENGINEERING\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Temporal features\n",
        "df['hour'] = df['recorded_at'].dt.hour\n",
        "df['day_of_week'] = df['recorded_at'].dt.dayofweek\n",
        "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "df['month'] = df['recorded_at'].dt.month\n",
        "\n",
        "# Operational phase duration\n",
        "df['phase_duration_hours'] = (df['recorded_at'] - df['phase_started_at']).dt.total_seconds() / 3600\n",
        "\n",
        "# Rolling statistics (detect gradual changes that precede earthquakes)\n",
        "for window in [6, 12, 24]:\n",
        "    df[f'inj_temp_rolling_mean_{window}h'] = df['inj_temp'].rolling(window, min_periods=1).mean()\n",
        "    df[f'inj_temp_rolling_std_{window}h'] = df['inj_temp'].rolling(window, min_periods=1).std()\n",
        "    df[f'inj_whp_rolling_mean_{window}h'] = df['inj_whp'].rolling(window, min_periods=1).mean()\n",
        "    df[f'prod_flow_rolling_max_{window}h'] = df['prod_flow'].rolling(window, min_periods=1).max()\n",
        "\n",
        "# Rate of change (sudden spikes may precede earthquakes)\n",
        "df['inj_temp_change'] = df['inj_temp'].diff()\n",
        "df['inj_whp_change'] = df['inj_whp'].diff()\n",
        "df['cum_inj_energy_change'] = df['cum_inj_energy'].diff()\n",
        "df['prod_temp_change'] = df['prod_temp'].diff()\n",
        "\n",
        "# Pressure ratios and differences\n",
        "df['pressure_diff'] = df['inj_whp'] - df['prod_whp']\n",
        "df['temp_diff'] = df['inj_temp'] - df['prod_temp']\n",
        "\n",
        "# Energy efficiency metrics\n",
        "df['inj_energy_per_flow'] = df['inj_energy'] / (df['inj_flow'] + 1e-6)\n",
        "df['cooling_efficiency'] = df['cooling_energy'] / (df['inj_energy'] + 1e-6)\n",
        "\n",
        "# Cumulative stress indicators\n",
        "df['cum_energy_normalized'] = df['cum_inj_energy'] / (df['cum_volume'] + 1e-6)\n",
        "\n",
        "# Interaction features\n",
        "df['temp_pressure_interaction'] = df['inj_temp'] * df['inj_whp']\n",
        "df['flow_pressure_interaction'] = df['inj_flow'] * df['inj_whp']\n",
        "\n",
        "print(f\"  ✓ Created {len([c for c in df.columns if any(x in c for x in ['rolling', 'change', 'diff', 'ratio', 'efficiency', 'interaction'])])} engineered features\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: FEATURE SELECTION & DATA PREP\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: DATA PREPARATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Exclude leaky and redundant features\n",
        "exclude_cols = [\n",
        "    'recorded_at', 'phase_started_at', 'phase_production_ended_at',\n",
        "    'phase_ended_at', 'occurred_at', 'target',\n",
        "    'hourly_seismicity_rate',  # This is the target proxy\n",
        "    'rounded', 'adjusted'  # Unclear meaning, possibly leaky\n",
        "]\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "X = df[feature_cols].copy()\n",
        "y = df['target'].astype(int)\n",
        "\n",
        "# Imputation\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "print(f\"\\n  Features breakdown:\")\n",
        "print(f\"    Total: {X.shape[1]}\")\n",
        "print(f\"    Numeric: {len(numeric_cols)}\")\n",
        "print(f\"    Categorical: {len(categorical_cols)}\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if X[col].isna().sum() > 0:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "for col in categorical_cols:\n",
        "    X[col] = X[col].astype(str).fillna('missing')\n",
        "\n",
        "# Remove any infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "for col in numeric_cols:\n",
        "    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "# Stratified random split (better than temporal for this imbalanced case)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n  Train/Test split:\")\n",
        "print(f\"    Train: {len(X_train):,} samples ({y_train.mean()*100:.4f}% positive)\")\n",
        "print(f\"    Test:  {len(X_test):,} samples ({y_test.mean()*100:.4f}% positive)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: HYPERPARAMETER-OPTIMIZED MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: TRAINING OPTIMIZED MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cat_features = [i for i, col in enumerate(X.columns) if col in categorical_cols]\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1500,\n",
        "    learning_rate=0.03,\n",
        "    depth=8,\n",
        "    l2_leaf_reg=5,\n",
        "    min_data_in_leaf=10,\n",
        "    max_leaves=64,\n",
        "    random_seed=42,\n",
        "    verbose=150,\n",
        "    eval_metric='AUC',\n",
        "    early_stopping_rounds=100,\n",
        "    auto_class_weights='Balanced',\n",
        "    bootstrap_type='MVS',  # Better for imbalanced data\n",
        "    subsample=0.8,\n",
        "    border_count=254\n",
        ")\n",
        "\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
        "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
        "\n",
        "print(\"\\n  Training in progress...\")\n",
        "model.fit(train_pool, eval_set=test_pool, use_best_model=True, plot=False)\n",
        "\n",
        "print(f\"\\n  ✓ Training complete!\")\n",
        "print(f\"    Best iteration: {model.get_best_iteration()}\")\n",
        "print(f\"    Total trees: {model.tree_count_}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: COMPREHENSIVE EVALUATION\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: MODEL PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Predictions\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Find optimal threshold\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "optimal_idx = np.argmax(f1_scores[:-1])  # Exclude last element\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "ap_score = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\n📊 MODEL METRICS:\")\n",
        "print(f\"  ROC AUC:              {auc_score:.6f}\")\n",
        "print(f\"  Average Precision:    {ap_score:.6f}\")\n",
        "print(f\"  Optimal Threshold:    {optimal_threshold:.4f} (F1-optimized)\")\n",
        "\n",
        "# Evaluate with both thresholds\n",
        "print(f\"\\n📋 CLASSIFICATION REPORT (Default threshold=0.5):\")\n",
        "print(classification_report(y_test, y_pred_default,\n",
        "                          target_names=['No Earthquake', 'Earthquake'],\n",
        "                          zero_division=0))\n",
        "\n",
        "print(f\"\\n📋 CLASSIFICATION REPORT (Optimal threshold={optimal_threshold:.4f}):\")\n",
        "print(classification_report(y_test, y_pred_optimal,\n",
        "                          target_names=['No Earthquake', 'Earthquake'],\n",
        "                          zero_division=0))\n",
        "\n",
        "# Confusion matrices\n",
        "cm_default = confusion_matrix(y_test, y_pred_default)\n",
        "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
        "\n",
        "print(f\"\\n📉 CONFUSION MATRIX (Default threshold):\")\n",
        "tn, fp, fn, tp = cm_default.ravel()\n",
        "print(f\"  True Negatives:  {tn:,}\")\n",
        "print(f\"  False Positives: {fp:,}\")\n",
        "print(f\"  False Negatives: {fn:,}  ⚠️ CRITICAL: Missed earthquakes\")\n",
        "print(f\"  True Positives:  {tp:,}\")\n",
        "\n",
        "print(f\"\\n📉 CONFUSION MATRIX (Optimal threshold={optimal_threshold:.4f}):\")\n",
        "tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
        "print(f\"  True Negatives:  {tn_opt:,}\")\n",
        "print(f\"  False Positives: {fp_opt:,}\")\n",
        "print(f\"  False Negatives: {fn_opt:,}  ⚠️ CRITICAL: Missed earthquakes\")\n",
        "print(f\"  True Positives:  {tp_opt:,}\")\n",
        "\n",
        "# Operational metrics\n",
        "print(f\"\\n💡 OPERATIONAL INSIGHTS:\")\n",
        "print(f\"\\n  With default threshold (0.5):\")\n",
        "print(f\"    Earthquake Detection Rate: {tp/(tp+fn)*100:.2f}% (caught {tp}/{tp+fn})\")\n",
        "print(f\"    False Alarm Rate: {fp/(tn+fp)*100:.2f}%\")\n",
        "print(f\"    Precision: {tp/(tp+fp)*100:.2f}% (when we predict earthquake)\")\n",
        "print(f\"\\n  With optimal threshold ({optimal_threshold:.4f}):\")\n",
        "print(f\"    Earthquake Detection Rate: {tp_opt/(tp_opt+fn_opt)*100:.2f}% (caught {tp_opt}/{tp_opt+fn_opt})\")\n",
        "print(f\"    False Alarm Rate: {fp_opt/(tn_opt+fp_opt)*100:.2f}%\")\n",
        "print(f\"    Precision: {tp_opt/(tp_opt+fp_opt)*100:.2f}%\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\n🔝 TOP 20 MOST IMPORTANT FEATURES:\")\n",
        "print(\"-\" * 80)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(20).iterrows():\n",
        "    print(f\"  {row['feature']:50s} {row['importance']:8.2f}\")\n",
        "\n",
        "# Additional analysis for different thresholds\n",
        "print(f\"\\n🎚️ THRESHOLD SENSITIVITY ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "test_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, optimal_threshold]\n",
        "print(f\"{'Threshold':>10} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Detected':>10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for thresh in sorted(set(test_thresholds)):\n",
        "    y_pred_t = (y_pred_proba >= thresh).astype(int)\n",
        "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
        "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
        "\n",
        "    prec = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0\n",
        "    rec = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
        "\n",
        "    print(f\"{thresh:>10.4f} {prec:>10.4f} {rec:>10.4f} {f1:>10.4f} {tp_t:>10d}/{tp_t+fn_t}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: SAVE OUTPUTS\n",
        "# ==============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: SAVING OUTPUTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save model\n",
        "model_path = '/content/drive/MyDrive/SoilDataset/earthquake_final_model.cbm'\n",
        "model.save_model(model_path)\n",
        "print(f\"  ✓ Model: {model_path}\")\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance.to_csv('/content/drive/MyDrive/SoilDataset/feature_importance_final.csv', index=False)\n",
        "print(f\"  ✓ Feature importance: feature_importance_final.csv\")\n",
        "\n",
        "# Save predictions with probabilities\n",
        "results_df = pd.DataFrame({\n",
        "    'actual': y_test,\n",
        "    'predicted_proba': y_pred_proba,\n",
        "    'predicted_default': y_pred_default,\n",
        "    'predicted_optimal': y_pred_optimal\n",
        "})\n",
        "results_df.to_csv('/content/drive/MyDrive/SoilDataset/predictions_final.csv', index=False)\n",
        "print(f\"  ✓ Predictions: predictions_final.csv\")\n",
        "\n",
        "# Save threshold recommendation\n",
        "with open('/content/drive/MyDrive/SoilDataset/recommended_threshold.txt', 'w') as f:\n",
        "    f.write(f\"Recommended Threshold: {optimal_threshold:.6f}\\n\")\n",
        "    f.write(f\"ROC AUC: {auc_score:.6f}\\n\")\n",
        "    f.write(f\"Average Precision: {ap_score:.6f}\\n\")\n",
        "    f.write(f\"\\nAt this threshold:\\n\")\n",
        "    f.write(f\"  Detection Rate: {tp_opt/(tp_opt+fn_opt)*100:.2f}%\\n\")\n",
        "    f.write(f\"  False Alarm Rate: {fp_opt/(tn_opt+fp_opt)*100:.2f}%\\n\")\n",
        "    f.write(f\"  Precision: {tp_opt/(tp_opt+fp_opt)*100:.2f}%\\n\")\n",
        "print(f\"  ✓ Threshold recommendation: recommended_threshold.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ OPTIMIZATION COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary = {\n",
        "    'Total Records': len(df),\n",
        "    'Features': X.shape[1],\n",
        "    'ROC AUC': f\"{auc_score:.6f}\",\n",
        "    'Avg Precision': f\"{ap_score:.4f}\",\n",
        "    'Best Threshold': f\"{optimal_threshold:.4f}\",\n",
        "    'Detection Rate': f\"{tp_opt/(tp_opt+fn_opt)*100:.1f}%\",\n",
        "    'Missed Events': fn_opt,\n",
        "    'False Alarms': fp_opt\n",
        "}\n",
        "\n",
        "print(\"\\n📊 FINAL SUMMARY:\")\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key:20s}: {value}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g8w6bNBB4Xr0",
        "outputId": "6aa4ab7a-de18-46bb-ff77-f951b52ad1b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🌍 OPTIMIZED EARTHQUAKE PREDICTION SYSTEM v2\n",
            "================================================================================\n",
            "\n",
            "✓ Loaded 696,275 records with 30 features\n",
            "\n",
            "================================================================================\n",
            "STEP 1: DATA CLEANING\n",
            "================================================================================\n",
            "  pgv_max: Replacing 689,407 sentinel values with NaN\n",
            "  magnitude: Replacing 689,407 sentinel values with NaN\n",
            "  hourly_seismicity_rate: Replacing 689,407 sentinel values with NaN\n",
            "  ✓ Data cleaned and sorted by time\n",
            "\n",
            "================================================================================\n",
            "STEP 2: TARGET VARIABLE CREATION\n",
            "================================================================================\n",
            "\n",
            "  Strategy: Binary classification of seismic events\n",
            "  Definition: magnitude >= 2.0 OR hourly_seismicity_rate > 0\n",
            "\n",
            "  Class 0 (No earthquake): 695,615 (99.91%)\n",
            "  Class 1 (Earthquake):    660 (0.09%)\n",
            "  Imbalance ratio: 1054.0:1\n",
            "\n",
            "================================================================================\n",
            "STEP 3: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "  ✓ Created 22 engineered features\n",
            "\n",
            "================================================================================\n",
            "STEP 4: DATA PREPARATION\n",
            "================================================================================\n",
            "\n",
            "  Features breakdown:\n",
            "    Total: 50\n",
            "    Numeric: 49\n",
            "    Categorical: 1\n",
            "\n",
            "  Train/Test split:\n",
            "    Train: 557,020 samples (0.0948% positive)\n",
            "    Test:  139,255 samples (0.0948% positive)\n",
            "\n",
            "================================================================================\n",
            "STEP 5: TRAINING OPTIMIZED MODEL\n",
            "================================================================================\n",
            "\n",
            "  Training in progress...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CatBoostError",
          "evalue": "catboost/private/libs/options/catboost_options.cpp:998: max_leaves option works only with lossguide tree growing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1193214.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n  Training in progress...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n  ✓ Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5245\u001b[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[1;32m   5246\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y may be None only when X is an instance of catboost.Pool or string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2395\u001b[0;31m             train_params = self._prepare_train_params(\n\u001b[0m\u001b[1;32m   2396\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m                 \u001b[0mpairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[1;32m   2319\u001b[0m         \u001b[0m_check_param_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_params_type_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m         \u001b[0m_check_train_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_fraction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCatBoostError\u001b[0m: catboost/private/libs/options/catboost_options.cpp:998: max_leaves option works only with lossguide tree growing"
          ]
        }
      ]
    }
  ]
}